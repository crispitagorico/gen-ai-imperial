{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: RL Training Methods for Language Models\n",
    "\n",
    "**MSc Course -- Generative Models in Finance**\n",
    "\n",
    "This notebook walks through the key training paradigms used to align and improve language models after pretraining:\n",
    "\n",
    "| Section | Method | Key Idea |\n",
    "|---------|--------|----------|\n",
    "| 1 | **SFT** | Supervised fine-tuning on instruction-response pairs |\n",
    "| 2 | **REINFORCE** | Classic policy-gradient with a simple reward heuristic |\n",
    "| 3 | **PPO** | Proximal Policy Optimisation with GAE and clipped objective |\n",
    "| 4 | **GRPO** | Group Relative Policy Optimisation (critic-free) |\n",
    "| 5 | **Bradley-Terry Reward Model** | Learn a reward model from preference data |\n",
    "\n",
    "All models are tiny (~1M parameters) and train on CPU in under 2 minutes.\n",
    "\n",
    "**Dependencies:** `torch`, `numpy`, `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Infrastructure: Tiny GPT-2 Model and Tokeniser\n",
    "\n",
    "We build a minimal GPT-2-style transformer from scratch. It has:\n",
    "- A small vocabulary (character-level + special tokens)\n",
    "- 4 transformer layers, 4 attention heads, embedding dim 128\n",
    "- ~1M parameters\n",
    "\n",
    "This is shared across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Simple character-level tokeniser\n",
    "# ---------------------------------------------------------------------------\n",
    "class CharTokeniser:\n",
    "    \"\"\"Character-level tokeniser with special tokens.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Printable ASCII 32-126 plus special tokens\n",
    "        chars = [chr(i) for i in range(32, 127)]  # 95 chars\n",
    "        self.special = {\"<pad>\": 0, \"<bos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.char2id = {**self.special}\n",
    "        for i, c in enumerate(chars):\n",
    "            self.char2id[c] = len(self.special) + i\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}\n",
    "        self.vocab_size = len(self.char2id)\n",
    "        self.pad_id = self.special[\"<pad>\"]\n",
    "        self.bos_id = self.special[\"<bos>\"]\n",
    "        self.eos_id = self.special[\"<eos>\"]\n",
    "\n",
    "    def encode(self, text: str, add_bos=True, add_eos=True) -> list[int]:\n",
    "        ids = []\n",
    "        if add_bos:\n",
    "            ids.append(self.bos_id)\n",
    "        for c in text:\n",
    "            ids.append(self.char2id.get(c, self.special[\"<unk>\"]))\n",
    "        if add_eos:\n",
    "            ids.append(self.eos_id)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        chars = []\n",
    "        for i in ids:\n",
    "            tok = self.id2char.get(i, \"\")\n",
    "            if tok in (\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"):\n",
    "                continue\n",
    "            chars.append(tok)\n",
    "        return \"\".join(chars)\n",
    "\n",
    "\n",
    "tokeniser = CharTokeniser()\n",
    "VOCAB_SIZE = tokeniser.vocab_size\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Example encode: {tokeniser.encode('hello')}\")\n",
    "print(f\"Example decode: {tokeniser.decode(tokeniser.encode('hello'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Tiny GPT-2 Model\n",
    "# ---------------------------------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(max_len, max_len)).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)  # (B, T, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        # Reshape to (B, n_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        # Attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = att @ v  # (B, n_heads, T, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, max_len, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT2(nn.Module):\n",
    "    \"\"\"Minimal GPT-2 style autoregressive language model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 4,\n",
    "        max_len: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, n_heads, max_len, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns logits of shape (B, T, vocab_size).\"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.max_len, f\"Sequence length {T} > max_len {self.max_len}\"\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # (1, T)\n",
    "        x = self.drop(self.tok_emb(idx) + self.pos_emb(pos))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Autoregressive generation via sampling.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_len :]\n",
    "            logits = self(idx_cond)[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_tok], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Instantiate and inspect\n",
    "_test_model = TinyGPT2(VOCAB_SIZE)\n",
    "print(f\"TinyGPT2 parameters: {count_parameters(_test_model):,}\")\n",
    "del _test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Supervised Fine-Tuning (SFT)\n",
    "\n",
    "SFT is the first step after pretraining. We take a set of **(instruction, response)** pairs and\n",
    "fine-tune the model with **teacher-forced cross-entropy loss**: at each position $t$ the model\n",
    "predicts the next token given the ground-truth prefix.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{SFT}} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log p_\\theta(y_t \\mid y_{<t})$$\n",
    "\n",
    "We train on a small synthetic dataset of instruction-response pairs relevant to finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1a. SFT Dataset\n",
    "# ---------------------------------------------------------------------------\n",
    "SFT_DATA = [\n",
    "    (\"What is a bond?\", \"A bond is a fixed-income instrument representing a loan made by an investor to a borrower.\"),\n",
    "    (\"Define stock.\", \"A stock is a security that represents ownership of a fraction of a corporation.\"),\n",
    "    (\"What is volatility?\", \"Volatility measures the degree of variation in a trading price over time.\"),\n",
    "    (\"Explain diversification.\", \"Diversification is a risk management strategy mixing a variety of investments.\"),\n",
    "    (\"What is a derivative?\", \"A derivative is a financial contract whose value depends on an underlying asset.\"),\n",
    "    (\"Define yield.\", \"Yield is the income return on an investment, typically expressed as a percentage.\"),\n",
    "    (\"What is liquidity?\", \"Liquidity refers to how quickly an asset can be converted to cash without loss.\"),\n",
    "    (\"Explain hedging.\", \"Hedging is an investment to reduce the risk of adverse price movements in an asset.\"),\n",
    "    (\"What is a put option?\", \"A put option gives the holder the right to sell an asset at a specified price.\"),\n",
    "    (\"Define alpha.\", \"Alpha is the excess return of an investment relative to its benchmark index.\"),\n",
    "    (\"What is a call option?\", \"A call option gives the holder the right to buy an asset at a specified price.\"),\n",
    "    (\"Explain leverage.\", \"Leverage is the use of borrowed capital to increase the potential return on investment.\"),\n",
    "    (\"What is beta?\", \"Beta measures the sensitivity of an asset's returns to the overall market returns.\"),\n",
    "    (\"Define portfolio.\", \"A portfolio is a collection of financial investments like stocks, bonds, and cash.\"),\n",
    "    (\"What is arbitrage?\", \"Arbitrage is the simultaneous purchase and sale of an asset to profit from price differences.\"),\n",
    "    (\"Explain short selling.\", \"Short selling involves borrowing shares and selling them, hoping to buy back at lower price.\"),\n",
    "]\n",
    "\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data, tokeniser, max_len=128):\n",
    "        self.samples = []\n",
    "        for instruction, response in data:\n",
    "            text = f\"Q: {instruction} A: {response}\"\n",
    "            ids = tokeniser.encode(text, add_bos=True, add_eos=True)\n",
    "            ids = ids[:max_len]  # truncate\n",
    "            self.samples.append(torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_pad(batch, pad_id=0):\n",
    "    max_len = max(len(s) for s in batch)\n",
    "    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n",
    "    for i, s in enumerate(batch):\n",
    "        padded[i, : len(s)] = s\n",
    "    return padded\n",
    "\n",
    "\n",
    "sft_dataset = SFTDataset(SFT_DATA, tokeniser)\n",
    "sft_loader = DataLoader(sft_dataset, batch_size=8, shuffle=True, collate_fn=collate_pad)\n",
    "print(f\"SFT dataset size: {len(sft_dataset)} samples\")\n",
    "print(f\"Example (decoded): {tokeniser.decode(sft_dataset[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1b. SFT Training Loop\n",
    "# ---------------------------------------------------------------------------\n",
    "sft_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "sft_optimiser = torch.optim.AdamW(sft_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "NUM_SFT_EPOCHS = 60\n",
    "sft_losses = []\n",
    "\n",
    "sft_model.train()\n",
    "for epoch in range(NUM_SFT_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for batch in sft_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        # Teacher forcing: input = tokens[:-1], target = tokens[1:]\n",
    "        inputs = batch[:, :-1]\n",
    "        targets = batch[:, 1:]\n",
    "        logits = sft_model(inputs)  # (B, T-1, V)\n",
    "        # Flatten for cross-entropy; ignore padding (id=0)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1),\n",
    "            ignore_index=tokeniser.pad_id,\n",
    "        )\n",
    "        sft_optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(sft_model.parameters(), 1.0)\n",
    "        sft_optimiser.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    sft_losses.append(avg_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{NUM_SFT_EPOCHS}  loss={avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nSFT training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1c. SFT Training Loss Curve and Sample Generation\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3.5))\n",
    "ax.plot(sft_losses, linewidth=1.5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax.set_title(\"SFT Training Loss\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate a sample\n",
    "sft_model.eval()\n",
    "prompt = \"Q: What is a bond? A:\"\n",
    "prompt_ids = torch.tensor([tokeniser.encode(prompt, add_bos=True, add_eos=False)], device=DEVICE)\n",
    "gen_ids = sft_model.generate(prompt_ids, max_new_tokens=60, temperature=0.7)\n",
    "print(f\"Prompt:    {prompt}\")\n",
    "print(f\"Generated: {tokeniser.decode(gen_ids[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. REINFORCE (Policy Gradient)\n",
    "\n",
    "REINFORCE treats the language model as a **policy** $\\pi_\\theta$ that generates token sequences\n",
    "(actions). A scalar **reward** $R$ is assigned to each complete sequence.\n",
    "\n",
    "The policy gradient estimator is:\n",
    "\n",
    "$$\\nabla_\\theta J = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=1}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot (R(\\tau) - b)\\right]$$\n",
    "\n",
    "where $b$ is an optional **baseline** (e.g. the running mean reward) to reduce variance.\n",
    "\n",
    "**Reward heuristic:** We use a simple reward that encourages the model to generate sequences\n",
    "that (a) contain a target word like \"profit\" and (b) have moderate length (penalising too short\n",
    "or too long outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2a. Reward function and rollout generation\n",
    "# ---------------------------------------------------------------------------\n",
    "TARGET_WORD = \"profit\"\n",
    "\n",
    "\n",
    "def compute_reward(text: str) -> float:\n",
    "    \"\"\"Simple heuristic reward for a generated text.\n",
    "    +2.0 if text contains the target word 'profit'\n",
    "    Length penalty: -0.02 * |len - 40|  (prefer ~40 chars)\n",
    "    Small bonus for ending with a period.\n",
    "    \"\"\"\n",
    "    r = 0.0\n",
    "    if TARGET_WORD in text.lower():\n",
    "        r += 2.0\n",
    "    # Length penalty: prefer around 40 characters\n",
    "    r -= 0.02 * abs(len(text) - 40)\n",
    "    # Punctuation bonus\n",
    "    if text.strip().endswith(\".\"):\n",
    "        r += 0.5\n",
    "    return r\n",
    "\n",
    "\n",
    "def generate_with_logprobs(\n",
    "    model: TinyGPT2,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate tokens and collect log-probabilities for each sampled token.\n",
    "    Returns (generated_ids [B, T+max_new], log_probs [B, max_new]).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_log_probs = []\n",
    "    idx = prompt_ids.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.max_len :]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        next_tok = dist.sample()\n",
    "        all_log_probs.append(dist.log_prob(next_tok))\n",
    "        idx = torch.cat([idx, next_tok.unsqueeze(1)], dim=1)\n",
    "    log_probs = torch.stack(all_log_probs, dim=1)  # (B, max_new_tokens)\n",
    "    return idx, log_probs\n",
    "\n",
    "\n",
    "# Quick test\n",
    "rl_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "# Copy SFT weights as starting point\n",
    "rl_model.load_state_dict(sft_model.state_dict())\n",
    "_p = torch.tensor([tokeniser.encode(\"Q: Explain profit. A:\", add_bos=True, add_eos=False)], device=DEVICE)\n",
    "_gen, _lp = generate_with_logprobs(rl_model, _p, max_new_tokens=20)\n",
    "print(f\"Generated: {tokeniser.decode(_gen[0].tolist())}\")\n",
    "print(f\"Log-prob shape: {_lp.shape}, sum: {_lp.sum().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2b. REINFORCE training (no baseline)\n",
    "# ---------------------------------------------------------------------------\n",
    "def reinforce_step(\n",
    "    model: TinyGPT2,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    batch_size: int,\n",
    "    temperature: float = 1.0,\n",
    "    baseline: float = 0.0,\n",
    ") -> tuple[float, float, list[float]]:\n",
    "    \"\"\"One REINFORCE gradient step.\n",
    "    Returns (mean_reward, policy_loss, list_of_per_sample_grads_norms).\n",
    "    \"\"\"\n",
    "    # Expand prompt for batch\n",
    "    prompts = prompt_ids.expand(batch_size, -1)\n",
    "\n",
    "    # Generate with gradients through log-probs\n",
    "    model.eval()  # keep dropout off for generation\n",
    "    all_log_probs = []\n",
    "    idx = prompts.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.max_len :]\n",
    "        logits = model(idx_cond)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        next_tok = dist.sample()\n",
    "        all_log_probs.append(dist.log_prob(next_tok))\n",
    "        idx = torch.cat([idx, next_tok.unsqueeze(1).detach()], dim=1)\n",
    "\n",
    "    log_probs = torch.stack(all_log_probs, dim=1)  # (B, max_new_tokens)\n",
    "\n",
    "    # Compute rewards\n",
    "    rewards = []\n",
    "    for i in range(batch_size):\n",
    "        gen_text = tokeniser.decode(idx[i].tolist())\n",
    "        rewards.append(compute_reward(gen_text))\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # REINFORCE loss: -E[log_prob * (R - baseline)]\n",
    "    advantages = rewards_t - baseline\n",
    "    per_sample_loss = -(log_probs.sum(dim=1) * advantages)\n",
    "    loss = per_sample_loss.mean()\n",
    "\n",
    "    return loss, rewards_t.mean().item(), rewards\n",
    "\n",
    "\n",
    "print(\"REINFORCE step function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2c. Train with REINFORCE -- compare with/without baseline\n",
    "# ---------------------------------------------------------------------------\n",
    "def train_reinforce(use_baseline: bool, n_steps: int = 80, batch_size: int = 16, lr: float = 1e-4):\n",
    "    model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "    model.load_state_dict(sft_model.state_dict())\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    prompt = \"Q: Explain profit. A:\"\n",
    "    prompt_ids = torch.tensor(\n",
    "        [tokeniser.encode(prompt, add_bos=True, add_eos=False)], device=DEVICE\n",
    "    )\n",
    "\n",
    "    reward_history = []\n",
    "    grad_norm_history = []\n",
    "    running_baseline = 0.0\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        baseline_val = running_baseline if use_baseline else 0.0\n",
    "        loss, mean_r, _ = reinforce_step(\n",
    "            model, prompt_ids, max_new_tokens=30, batch_size=batch_size,\n",
    "            temperature=1.0, baseline=baseline_val,\n",
    "        )\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        # Record gradient norm\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        grad_norm_history.append(total_norm.item())\n",
    "        optimiser.step()\n",
    "\n",
    "        reward_history.append(mean_r)\n",
    "        running_baseline = 0.95 * running_baseline + 0.05 * mean_r\n",
    "\n",
    "        if (step + 1) % 20 == 0:\n",
    "            tag = \"baseline\" if use_baseline else \"no baseline\"\n",
    "            print(f\"  [{tag}] step {step+1:3d}  reward={mean_r:.3f}  grad_norm={total_norm:.3f}\")\n",
    "\n",
    "    return reward_history, grad_norm_history, model\n",
    "\n",
    "\n",
    "print(\"Training REINFORCE without baseline...\")\n",
    "rewards_no_bl, grads_no_bl, _ = train_reinforce(use_baseline=False)\n",
    "print(\"\\nTraining REINFORCE with baseline...\")\n",
    "rewards_bl, grads_bl, reinforce_model = train_reinforce(use_baseline=True)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2d. Visualise REINFORCE results\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Reward curves\n",
    "axes[0].plot(rewards_no_bl, alpha=0.7, label=\"No baseline\")\n",
    "axes[0].plot(rewards_bl, alpha=0.7, label=\"With baseline\")\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Mean Reward\")\n",
    "axes[0].set_title(\"REINFORCE: Reward over Training\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm comparison\n",
    "axes[1].plot(grads_no_bl, alpha=0.7, label=\"No baseline\")\n",
    "axes[1].plot(grads_bl, alpha=0.7, label=\"With baseline\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Gradient Norm\")\n",
    "axes[1].set_title(\"REINFORCE: Gradient Variance\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "std_no_bl = np.std(grads_no_bl)\n",
    "std_bl = np.std(grads_bl)\n",
    "print(f\"\\nGrad norm std (no baseline):   {std_no_bl:.3f}\")\n",
    "print(f\"Grad norm std (with baseline): {std_bl:.3f}\")\n",
    "reduction = (1 - std_bl / std_no_bl) * 100\n",
    "print(f\"-> The baseline reduces gradient norm std by {reduction:.0f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2e. Show sample generations from the REINFORCE-trained model\n",
    "# ---------------------------------------------------------------------------\n",
    "reinforce_model.eval()\n",
    "prompt = \"Q: Explain profit. A:\"\n",
    "prompt_ids = torch.tensor(\n",
    "    [tokeniser.encode(prompt, add_bos=True, add_eos=False)], device=DEVICE\n",
    ")\n",
    "print(\"Sample generations from REINFORCE model:\\n\")\n",
    "for i in range(5):\n",
    "    gen = reinforce_model.generate(prompt_ids, max_new_tokens=40, temperature=0.8)\n",
    "    text = tokeniser.decode(gen[0].tolist())\n",
    "    r = compute_reward(text)\n",
    "    print(f\"  [{i+1}] (R={r:.2f}) {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. PPO from Scratch\n",
    "\n",
    "Proximal Policy Optimisation (PPO) improves on vanilla REINFORCE with:\n",
    "\n",
    "1. **Clipped surrogate objective** -- prevents the policy from changing too much:\n",
    "   $$L^{\\text{CLIP}} = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t,\\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "   where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$\n",
    "\n",
    "2. **Generalised Advantage Estimation (GAE)** -- a biased but lower-variance advantage:\n",
    "   $$\\hat{A}_t^{\\text{GAE}} = \\sum_{l=0}^{T-t-1} (\\gamma\\lambda)^l \\delta_{t+l}, \\quad \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "3. **Value function** -- a critic that estimates the expected return from each state.\n",
    "\n",
    "4. **Multiple minibatch epochs** over the same rollout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3a. Value Network (Critic)\n",
    "# ---------------------------------------------------------------------------\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"Scalar value head on top of the transformer backbone.\n",
    "    Shares the backbone with the policy but has a separate linear head.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 128):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"hidden_states: (B, T, d_model) -> values: (B, T)\"\"\"\n",
    "        return self.head(hidden_states).squeeze(-1)\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Wraps TinyGPT2 (actor) and a ValueHead (critic).\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, **kwargs):\n",
    "        super().__init__()\n",
    "        self.backbone = TinyGPT2(vocab_size, d_model=d_model, **kwargs)\n",
    "        self.value_head = ValueHead(d_model)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor):\n",
    "        \"\"\"Returns (logits, values).\"\"\"\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.backbone.drop(self.backbone.tok_emb(idx) + self.backbone.pos_emb(pos))\n",
    "        for block in self.backbone.blocks:\n",
    "            x = block(x)\n",
    "        x = self.backbone.ln_f(x)  # (B, T, d_model)\n",
    "        logits = self.backbone.head(x)  # (B, T, V)\n",
    "        values = self.value_head(x)  # (B, T)\n",
    "        return logits, values\n",
    "\n",
    "    def get_logits(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        logits, _ = self(idx)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Quick check\n",
    "_ac = ActorCritic(VOCAB_SIZE)\n",
    "print(f\"ActorCritic parameters: {count_parameters(_ac):,}\")\n",
    "del _ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3b. Rollout collection\n",
    "# ---------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    actor_critic: ActorCritic,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    ") -> dict:\n",
    "    \"\"\"Generate sequences and collect data needed for PPO.\"\"\"\n",
    "    prompts = prompt_ids.expand(batch_size, -1)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    idx = prompts.clone()\n",
    "    all_log_probs = []\n",
    "    all_values = []\n",
    "    all_actions = []\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -actor_critic.backbone.max_len :]\n",
    "        logits, values = actor_critic(idx_cond)\n",
    "        # Take last position logits and value\n",
    "        last_logits = logits[:, -1, :] / temperature\n",
    "        last_value = values[:, -1]  # (B,)\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        all_actions.append(action)\n",
    "        all_log_probs.append(log_prob)\n",
    "        all_values.append(last_value)\n",
    "\n",
    "        idx = torch.cat([idx, action.unsqueeze(1)], dim=1)\n",
    "\n",
    "    # Compute terminal values (bootstrap = 0 for complete episodes)\n",
    "    # Compute per-sequence rewards\n",
    "    rewards_list = []\n",
    "    for i in range(batch_size):\n",
    "        gen_text = tokeniser.decode(idx[i].tolist())\n",
    "        rewards_list.append(compute_reward(gen_text))\n",
    "\n",
    "    # We treat the whole generation as one \"step\" with the reward at the end\n",
    "    # For token-level PPO, we assign reward only at the last token\n",
    "    token_rewards = torch.zeros(batch_size, max_new_tokens, device=DEVICE)\n",
    "    for i in range(batch_size):\n",
    "        token_rewards[i, -1] = rewards_list[i]\n",
    "\n",
    "    return {\n",
    "        \"sequences\": idx,  # (B, prompt_len + max_new_tokens)\n",
    "        \"actions\": torch.stack(all_actions, dim=1),  # (B, max_new_tokens)\n",
    "        \"log_probs\": torch.stack(all_log_probs, dim=1),  # (B, max_new_tokens)\n",
    "        \"values\": torch.stack(all_values, dim=1),  # (B, max_new_tokens)\n",
    "        \"token_rewards\": token_rewards,  # (B, max_new_tokens)\n",
    "        \"total_rewards\": torch.tensor(rewards_list, device=DEVICE),  # (B,)\n",
    "        \"prompt_len\": prompt_len,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Rollout collection function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3c. GAE Computation\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_gae(\n",
    "    token_rewards: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    gamma: float = 1.0,\n",
    "    lam: float = 0.95,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute Generalised Advantage Estimation.\n",
    "\n",
    "    Args:\n",
    "        token_rewards: (B, T) rewards at each token step\n",
    "        values: (B, T) value estimates from critic\n",
    "        gamma: discount factor\n",
    "        lam: GAE lambda parameter\n",
    "\n",
    "    Returns:\n",
    "        advantages: (B, T)\n",
    "        returns: (B, T) = advantages + values\n",
    "    \"\"\"\n",
    "    B, T = token_rewards.shape\n",
    "    advantages = torch.zeros_like(token_rewards)\n",
    "    last_gae = torch.zeros(B, device=token_rewards.device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_value = torch.zeros(B, device=token_rewards.device)  # terminal\n",
    "        else:\n",
    "            next_value = values[:, t + 1]\n",
    "        # TD error: delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "        delta = token_rewards[:, t] + gamma * next_value - values[:, t]\n",
    "        # GAE: A_t = delta_t + gamma * lambda * A_{t+1}\n",
    "        last_gae = delta + gamma * lam * last_gae\n",
    "        advantages[:, t] = last_gae\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# Quick test\n",
    "_r = torch.tensor([[0.0, 0.0, 1.0]])\n",
    "_v = torch.tensor([[0.1, 0.2, 0.3]])\n",
    "_adv, _ret = compute_gae(_r, _v, gamma=0.99, lam=0.95)\n",
    "print(f\"Test rewards: {_r}\")\n",
    "print(f\"Test values:  {_v}\")\n",
    "print(f\"GAE advantages: {_adv}\")\n",
    "print(f\"Returns:        {_ret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3d. PPO Clipped Objective\n",
    "# ---------------------------------------------------------------------------\n",
    "def ppo_loss(\n",
    "    new_log_probs: torch.Tensor,\n",
    "    old_log_probs: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    clip_eps: float = 0.2,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"PPO clipped surrogate objective.\n",
    "\n",
    "    L = min(r_t * A_t, clip(r_t, 1-eps, 1+eps) * A_t)\n",
    "\n",
    "    We negate because we want to maximise the objective (gradient ascent).\n",
    "    \"\"\"\n",
    "    # Importance sampling ratio\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)  # r_t(theta)\n",
    "    # Clipped ratio\n",
    "    clipped_ratio = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "    # Surrogate losses\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = clipped_ratio * advantages\n",
    "    # Take the minimum (pessimistic bound) and negate for minimisation\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    return policy_loss\n",
    "\n",
    "\n",
    "def value_loss(predicted_values: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Simple MSE loss for the value function.\"\"\"\n",
    "    return F.mse_loss(predicted_values, returns)\n",
    "\n",
    "\n",
    "print(\"PPO loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3e. PPO update step with minibatch epochs\n",
    "# ---------------------------------------------------------------------------\n",
    "def ppo_update(\n",
    "    actor_critic: ActorCritic,\n",
    "    optimiser: torch.optim.Optimizer,\n",
    "    rollout: dict,\n",
    "    n_epochs: int = 4,\n",
    "    minibatch_size: int = 8,\n",
    "    clip_eps: float = 0.2,\n",
    "    vf_coef: float = 0.5,\n",
    "    entropy_coef: float = 0.01,\n",
    "    gamma: float = 1.0,\n",
    "    lam: float = 0.95,\n",
    ") -> dict:\n",
    "    \"\"\"Perform PPO update over multiple epochs of minibatches.\"\"\"\n",
    "    sequences = rollout[\"sequences\"]\n",
    "    actions = rollout[\"actions\"]\n",
    "    old_log_probs = rollout[\"log_probs\"]\n",
    "    old_values = rollout[\"values\"]\n",
    "    token_rewards = rollout[\"token_rewards\"]\n",
    "    prompt_len = rollout[\"prompt_len\"]\n",
    "\n",
    "    # Compute GAE\n",
    "    advantages, returns = compute_gae(token_rewards, old_values, gamma=gamma, lam=lam)\n",
    "    # Normalise advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    B = sequences.shape[0]\n",
    "    T_gen = actions.shape[1]\n",
    "    total_policy_loss = 0.0\n",
    "    total_value_loss = 0.0\n",
    "    n_updates = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle indices\n",
    "        perm = torch.randperm(B)\n",
    "        for start in range(0, B, minibatch_size):\n",
    "            mb_idx = perm[start : start + minibatch_size]\n",
    "            mb_seq = sequences[mb_idx]  # (mb, total_len)\n",
    "            mb_actions = actions[mb_idx]  # (mb, T_gen)\n",
    "            mb_old_lp = old_log_probs[mb_idx]\n",
    "            mb_advantages = advantages[mb_idx]\n",
    "            mb_returns = returns[mb_idx]\n",
    "\n",
    "            # Forward pass through actor-critic\n",
    "            # We feed the full sequence and extract logits/values for the generated part\n",
    "            full_logits, full_values = actor_critic(mb_seq)\n",
    "            # Logits at positions [prompt_len-1 ... prompt_len+T_gen-2] predict tokens\n",
    "            # at positions [prompt_len ... prompt_len+T_gen-1]\n",
    "            gen_logits = full_logits[:, prompt_len - 1 : prompt_len - 1 + T_gen, :]\n",
    "            gen_values = full_values[:, prompt_len - 1 : prompt_len - 1 + T_gen]\n",
    "\n",
    "            # Compute new log probs\n",
    "            gen_log_probs_all = F.log_softmax(gen_logits, dim=-1)\n",
    "            new_log_probs = gen_log_probs_all.gather(\n",
    "                2, mb_actions.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            # Entropy bonus\n",
    "            entropy = -(F.softmax(gen_logits, dim=-1) * gen_log_probs_all).sum(-1).mean()\n",
    "\n",
    "            # Losses\n",
    "            p_loss = ppo_loss(new_log_probs, mb_old_lp, mb_advantages, clip_eps=clip_eps)\n",
    "            v_loss = value_loss(gen_values, mb_returns)\n",
    "            total_loss = p_loss + vf_coef * v_loss - entropy_coef * entropy\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor_critic.parameters(), 1.0)\n",
    "            optimiser.step()\n",
    "\n",
    "            total_policy_loss += p_loss.item()\n",
    "            total_value_loss += v_loss.item()\n",
    "            n_updates += 1\n",
    "\n",
    "    return {\n",
    "        \"policy_loss\": total_policy_loss / max(n_updates, 1),\n",
    "        \"value_loss\": total_value_loss / max(n_updates, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"PPO update function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3f. PPO Training Loop\n",
    "# ---------------------------------------------------------------------------\n",
    "ppo_ac = ActorCritic(VOCAB_SIZE).to(DEVICE)\n",
    "# Initialise from SFT weights for the backbone\n",
    "ppo_ac.backbone.load_state_dict(sft_model.state_dict())\n",
    "\n",
    "ppo_optimiser = torch.optim.Adam(ppo_ac.parameters(), lr=5e-5)\n",
    "\n",
    "prompt_text = \"Q: Explain profit. A:\"\n",
    "prompt_ids = torch.tensor(\n",
    "    [tokeniser.encode(prompt_text, add_bos=True, add_eos=False)], device=DEVICE\n",
    ")\n",
    "\n",
    "PPO_STEPS = 80\n",
    "ROLLOUT_BATCH = 32\n",
    "GEN_LEN = 30\n",
    "\n",
    "ppo_reward_history = []\n",
    "ppo_policy_loss_history = []\n",
    "ppo_value_loss_history = []\n",
    "\n",
    "print(\"Starting PPO training...\\n\")\n",
    "for step in range(PPO_STEPS):\n",
    "    # 1. Collect rollouts\n",
    "    rollout = collect_rollouts(\n",
    "        ppo_ac, prompt_ids, batch_size=ROLLOUT_BATCH,\n",
    "        max_new_tokens=GEN_LEN, temperature=1.0,\n",
    "    )\n",
    "    mean_reward = rollout[\"total_rewards\"].mean().item()\n",
    "    ppo_reward_history.append(mean_reward)\n",
    "\n",
    "    # 2. PPO update\n",
    "    losses = ppo_update(\n",
    "        ppo_ac, ppo_optimiser, rollout,\n",
    "        n_epochs=4, minibatch_size=8, clip_eps=0.2,\n",
    "        vf_coef=0.5, entropy_coef=0.01,\n",
    "    )\n",
    "    ppo_policy_loss_history.append(losses[\"policy_loss\"])\n",
    "    ppo_value_loss_history.append(losses[\"value_loss\"])\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Step {step+1:3d}/{PPO_STEPS}  \"\n",
    "            f\"reward={mean_reward:.3f}  \"\n",
    "            f\"pi_loss={losses['policy_loss']:.4f}  \"\n",
    "            f\"v_loss={losses['value_loss']:.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"\\nPPO training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3g. PPO Training Plots\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(ppo_reward_history, linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Mean Reward\")\n",
    "axes[0].set_title(\"PPO: Reward over Training\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ppo_policy_loss_history, linewidth=1.5, color=\"tab:orange\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Policy Loss\")\n",
    "axes[1].set_title(\"PPO: Clipped Policy Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ppo_value_loss_history, linewidth=1.5, color=\"tab:green\")\n",
    "axes[2].set_xlabel(\"Step\")\n",
    "axes[2].set_ylabel(\"Value Loss\")\n",
    "axes[2].set_title(\"PPO: Value Function Loss\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3h. PPO Sample Generations\n",
    "# ---------------------------------------------------------------------------\n",
    "ppo_ac.eval()\n",
    "print(\"Sample generations from PPO-trained model:\\n\")\n",
    "for i in range(5):\n",
    "    with torch.no_grad():\n",
    "        gen = ppo_ac.backbone.generate(prompt_ids, max_new_tokens=40, temperature=0.8)\n",
    "    text = tokeniser.decode(gen[0].tolist())\n",
    "    r = compute_reward(text)\n",
    "    print(f\"  [{i+1}] (R={r:.2f}) {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Group Relative Policy Optimisation (GRPO)\n",
    "\n",
    "GRPO (from DeepSeek-R1) is a **critic-free** alternative to PPO. For each prompt:\n",
    "\n",
    "1. Generate a **group** of $G$ responses from the current policy.\n",
    "2. Score each response with a reward function.\n",
    "3. Compute **group-normalised advantages**: $A_i = \\frac{R_i - \\mu_G}{\\sigma_G}$\n",
    "4. Update the policy with a clipped objective (like PPO) but using these advantages.\n",
    "\n",
    "Key benefit: no value network needed, reducing complexity and training instability.\n",
    "\n",
    "Additionally, GRPO includes a KL penalty to keep the policy close to a reference:\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_i|}\\sum_{t=1}^{|o_i|}\\left[\\min(r_t A_i,\\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_i) - \\beta\\, D_{KL}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4a. GRPO: Generate group and compute group-normalised advantages\n",
    "# ---------------------------------------------------------------------------\n",
    "def grpo_generate_group(\n",
    "    model: TinyGPT2,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    group_size: int,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    ") -> dict:\n",
    "    \"\"\"Generate a group of responses and compute group-normalised advantages.\"\"\"\n",
    "    prompts = prompt_ids.expand(group_size, -1)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    # Generate with log-probs (need gradients for update)\n",
    "    model.eval()\n",
    "    idx = prompts.clone()\n",
    "    all_log_probs = []\n",
    "    all_actions = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.max_len :]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        all_actions.append(action)\n",
    "        all_log_probs.append(log_prob)\n",
    "        idx = torch.cat([idx, action.unsqueeze(1)], dim=1)\n",
    "\n",
    "    # Compute rewards\n",
    "    rewards = []\n",
    "    for i in range(group_size):\n",
    "        text = tokeniser.decode(idx[i].tolist())\n",
    "        rewards.append(compute_reward(text))\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # Group-normalised advantages: A_i = (R_i - mean) / std\n",
    "    mean_r = rewards_t.mean()\n",
    "    std_r = rewards_t.std() + 1e-8\n",
    "    advantages = (rewards_t - mean_r) / std_r  # (G,)\n",
    "\n",
    "    return {\n",
    "        \"sequences\": idx,\n",
    "        \"actions\": torch.stack(all_actions, dim=1),\n",
    "        \"old_log_probs\": torch.stack(all_log_probs, dim=1),\n",
    "        \"advantages\": advantages,\n",
    "        \"rewards\": rewards_t,\n",
    "        \"prompt_len\": prompt_len,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"GRPO group generation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4b. GRPO Update Step\n",
    "# ---------------------------------------------------------------------------\n",
    "def grpo_update(\n",
    "    model: TinyGPT2,\n",
    "    ref_model: TinyGPT2,\n",
    "    optimiser: torch.optim.Optimizer,\n",
    "    group_data: dict,\n",
    "    clip_eps: float = 0.2,\n",
    "    beta_kl: float = 0.04,\n",
    "    n_epochs: int = 2,\n",
    ") -> float:\n",
    "    \"\"\"GRPO policy update with clipped objective and KL penalty.\"\"\"\n",
    "    sequences = group_data[\"sequences\"]\n",
    "    actions = group_data[\"actions\"]\n",
    "    old_log_probs = group_data[\"old_log_probs\"]\n",
    "    advantages = group_data[\"advantages\"]  # (G,)\n",
    "    prompt_len = group_data[\"prompt_len\"]\n",
    "    G, T_gen = actions.shape\n",
    "\n",
    "    total_loss_val = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass\n",
    "        logits = model(sequences)  # (G, total_len, V)\n",
    "        gen_logits = logits[:, prompt_len - 1 : prompt_len - 1 + T_gen, :]\n",
    "        gen_log_probs = F.log_softmax(gen_logits, dim=-1)\n",
    "        new_log_probs = gen_log_probs.gather(2, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Reference model log probs (for KL)\n",
    "        with torch.no_grad():\n",
    "            ref_logits = ref_model(sequences)\n",
    "            ref_gen_logits = ref_logits[:, prompt_len - 1 : prompt_len - 1 + T_gen, :]\n",
    "            ref_log_probs = F.log_softmax(ref_gen_logits, dim=-1)\n",
    "            ref_lp = ref_log_probs.gather(2, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Importance sampling ratio\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)  # (G, T_gen)\n",
    "        clipped_ratio = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "\n",
    "        # Expand advantages to token level: (G,) -> (G, T_gen)\n",
    "        adv_expanded = advantages.unsqueeze(1).expand_as(ratio)\n",
    "\n",
    "        surr1 = ratio * adv_expanded\n",
    "        surr2 = clipped_ratio * adv_expanded\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # KL divergence penalty (approx): D_KL = exp(ref_lp - new_lp) - (ref_lp - new_lp) - 1\n",
    "        log_ratio_kl = ref_lp - new_log_probs\n",
    "        kl = (torch.exp(log_ratio_kl) - log_ratio_kl - 1.0).mean()\n",
    "\n",
    "        loss = policy_loss + beta_kl * kl\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimiser.step()\n",
    "        total_loss_val += loss.item()\n",
    "\n",
    "    return total_loss_val / n_epochs\n",
    "\n",
    "\n",
    "print(\"GRPO update function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4c. GRPO Training Loop\n",
    "# ---------------------------------------------------------------------------\n",
    "grpo_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "grpo_model.load_state_dict(sft_model.state_dict())\n",
    "grpo_ref = copy.deepcopy(grpo_model)  # frozen reference\n",
    "grpo_ref.eval()\n",
    "for p in grpo_ref.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "grpo_optimiser = torch.optim.Adam(grpo_model.parameters(), lr=5e-5)\n",
    "\n",
    "GRPO_STEPS = 80\n",
    "GROUP_SIZE = 32\n",
    "\n",
    "grpo_reward_history = []\n",
    "\n",
    "print(\"Starting GRPO training...\\n\")\n",
    "for step in range(GRPO_STEPS):\n",
    "    group_data = grpo_generate_group(\n",
    "        grpo_model, prompt_ids, group_size=GROUP_SIZE,\n",
    "        max_new_tokens=GEN_LEN, temperature=1.0,\n",
    "    )\n",
    "    mean_r = group_data[\"rewards\"].mean().item()\n",
    "    grpo_reward_history.append(mean_r)\n",
    "\n",
    "    loss = grpo_update(\n",
    "        grpo_model, grpo_ref, grpo_optimiser, group_data,\n",
    "        clip_eps=0.2, beta_kl=0.04, n_epochs=2,\n",
    "    )\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step+1:3d}/{GRPO_STEPS}  reward={mean_r:.3f}  loss={loss:.4f}\")\n",
    "\n",
    "print(\"\\nGRPO training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4d. Compare PPO vs GRPO\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(ppo_reward_history, label=\"PPO\", linewidth=1.5)\n",
    "ax.plot(grpo_reward_history, label=\"GRPO\", linewidth=1.5)\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_title(\"PPO vs GRPO: Reward over Training\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PPO final reward (last 10 avg):  {np.mean(ppo_reward_history[-10:]):.3f}\")\n",
    "print(f\"GRPO final reward (last 10 avg): {np.mean(grpo_reward_history[-10:]):.3f}\")\n",
    "print(\"\\nGRPO achieves comparable rewards without needing a critic network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Bradley-Terry Reward Model\n",
    "\n",
    "In Sections 2--4 the reward was a simple heuristic. In practice, the reward signal comes\n",
    "from a **learned reward model** trained on human preference data.\n",
    "\n",
    "The **Bradley-Terry** model (Bradley & Terry, 1952) defines the probability that response $y_w$\n",
    "is preferred over $y_l$ as:\n",
    "\n",
    "$$P(y_w \\succ y_l) = \\sigma\\!\\big(r_\\phi(x, y_w) - r_\\phi(x, y_l)\\big)$$\n",
    "\n",
    "The loss is the negative log-likelihood of observed preferences:\n",
    "$$\\mathcal{L}_{\\text{BT}} = -\\mathbb{E}\\!\\left[\\log\\sigma\\!\\big(r_\\phi(x, y_w) - r_\\phi(x, y_l)\\big)\\right]$$\n",
    "\n",
    "Below we train a small reward model on synthetic finance QA preference pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5a. Preference Data and Reward Model Architecture\n",
    "# ---------------------------------------------------------------------------\n",
    "# Each tuple: (prompt, preferred_response, dispreferred_response)\n",
    "BT_PREFS = [\n",
    "    (\"What is a bond?\",\n",
    "     \"A bond is a fixed-income instrument representing a loan from investor to borrower.\",\n",
    "     \"A bond is some kind of financial thing.\"),\n",
    "    (\"Define stock.\",\n",
    "     \"A stock represents ownership of a fraction of a corporation.\",\n",
    "     \"Stock is stuff you buy.\"),\n",
    "    (\"What is volatility?\",\n",
    "     \"Volatility measures the degree of variation in trading price over time.\",\n",
    "     \"Volatility is when prices go up and down.\"),\n",
    "    (\"Explain diversification.\",\n",
    "     \"Diversification is a risk management strategy mixing various investments.\",\n",
    "     \"Diversification means buying different things.\"),\n",
    "    (\"What is a derivative?\",\n",
    "     \"A derivative is a financial contract whose value depends on an underlying asset.\",\n",
    "     \"A derivative is complicated finance stuff.\"),\n",
    "    (\"Define yield.\",\n",
    "     \"Yield is the income return on an investment expressed as a percentage.\",\n",
    "     \"Yield is the money you get.\"),\n",
    "    (\"What is liquidity?\",\n",
    "     \"Liquidity refers to how quickly an asset converts to cash without loss.\",\n",
    "     \"Liquidity is about cash.\"),\n",
    "    (\"Explain hedging.\",\n",
    "     \"Hedging is an investment to reduce the risk of adverse price movements.\",\n",
    "     \"Hedging is like insurance or something.\"),\n",
    "    (\"What is leverage?\",\n",
    "     \"Leverage is using borrowed capital to increase potential return on investment.\",\n",
    "     \"Leverage means borrowing money.\"),\n",
    "    (\"Define alpha.\",\n",
    "     \"Alpha is the excess return of an investment relative to its benchmark index.\",\n",
    "     \"Alpha is a greek letter used in finance.\"),\n",
    "    (\"What is arbitrage?\",\n",
    "     \"Arbitrage is the simultaneous purchase and sale of an asset to profit from price differences.\",\n",
    "     \"Arbitrage is free money in the market.\"),\n",
    "    (\"Explain short selling.\",\n",
    "     \"Short selling involves borrowing shares and selling them, hoping to buy back at a lower price.\",\n",
    "     \"Short selling is betting stocks go down.\"),\n",
    "]\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Small reward model: transformer backbone + scalar head.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int = 96, n_heads: int = 4,\n",
    "                 n_layers: int = 3, max_len: int = 128):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, n_heads, max_len, dropout=0.1)\n",
    "             for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.reward_head = nn.Linear(d_model, 1)\n",
    "        self.max_len = max_len\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns scalar reward for each sequence in the batch. Shape: (B, 1).\"\"\"\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        last_hidden = x[:, -1, :]\n",
    "        reward = self.reward_head(last_hidden)\n",
    "        return reward\n",
    "\n",
    "\n",
    "rm = RewardModel(VOCAB_SIZE).to(DEVICE)\n",
    "print(f\"Reward Model parameters: {count_parameters(rm):,}\")\n",
    "print(f\"Preference pairs: {len(BT_PREFS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5b. Bradley-Terry Training and Evaluation\n",
    "# ---------------------------------------------------------------------------\n",
    "# Tokenise preference pairs\n",
    "all_bt_data = []\n",
    "for prompt, y_w, y_l in BT_PREFS:\n",
    "    full_w = f\"Q: {prompt} A: {y_w}\"\n",
    "    full_l = f\"Q: {prompt} A: {y_l}\"\n",
    "    ids_w = tokeniser.encode(full_w, add_bos=True, add_eos=True)\n",
    "    ids_l = tokeniser.encode(full_l, add_bos=True, add_eos=True)\n",
    "    all_bt_data.append((ids_w, ids_l))\n",
    "\n",
    "train_bt = all_bt_data[:9]\n",
    "test_bt = all_bt_data[9:]\n",
    "\n",
    "\n",
    "def pad_pair(ids_w, ids_l, pad_id=0):\n",
    "    max_len = max(len(ids_w), len(ids_l))\n",
    "    w = ids_w + [pad_id] * (max_len - len(ids_w))\n",
    "    l = ids_l + [pad_id] * (max_len - len(ids_l))\n",
    "    return (\n",
    "        torch.tensor([w], dtype=torch.long, device=DEVICE),\n",
    "        torch.tensor([l], dtype=torch.long, device=DEVICE),\n",
    "    )\n",
    "\n",
    "\n",
    "rm_optimiser = torch.optim.Adam(rm.parameters(), lr=1e-3)\n",
    "BT_EPOCHS = 60\n",
    "bt_losses = []\n",
    "\n",
    "print(\"Training Bradley-Terry Reward Model...\\n\")\n",
    "for epoch in range(BT_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    perm = np.random.permutation(len(train_bt))\n",
    "    for i in perm:\n",
    "        ids_w, ids_l = train_bt[i]\n",
    "        tw, tl = pad_pair(ids_w, ids_l)\n",
    "        r_w = rm(tw)\n",
    "        r_l = rm(tl)\n",
    "        loss = -F.logsigmoid(r_w - r_l).mean()\n",
    "        rm_optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        rm_optimiser.step()\n",
    "        epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(train_bt)\n",
    "    bt_losses.append(avg_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{BT_EPOCHS}  loss={avg_loss:.4f}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "rm.eval()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3.5))\n",
    "ax.plot(bt_losses, linewidth=1.5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Bradley-Terry Loss\")\n",
    "ax.set_title(\"Reward Model: Training Loss\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy on train and test\n",
    "for label, data in [(\"Train\", train_bt), (\"Test\", test_bt)]:\n",
    "    correct = sum(\n",
    "        1 for ids_w, ids_l in data\n",
    "        for tw, tl in [pad_pair(ids_w, ids_l)]\n",
    "        if rm(tw).item() > rm(tl).item()\n",
    "    )\n",
    "    print(f\"{label} accuracy: {correct}/{len(data)} = {correct/len(data):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Method | Requires Reward Model? | Requires Critic? | Key Mechanism |\n",
    "|--------|----------------------|------------------|---------------|\n",
    "| **SFT** | No | No | Teacher-forced cross-entropy on demonstrations |\n",
    "| **REINFORCE** | Yes (or heuristic) | No | $\\nabla J = \\mathbb{E}[\\nabla\\log\\pi \\cdot R]$, high variance |\n",
    "| **PPO** | Yes (or heuristic) | Yes (value fn) | Clipped ratio + GAE advantages |\n",
    "| **GRPO** | Yes (or heuristic) | **No** | Group-normalised advantages, no critic |\n",
    "| **Bradley-Terry RM** | N/A (this *is* the RM) | No | Pairwise preference loss |\n",
    "\n",
    "- SFT provides the foundation; RL methods refine behaviour towards a reward signal.\n",
    "- REINFORCE is simple but has high variance; baselines help.\n",
    "- PPO stabilises training via clipping and GAE, but requires a critic.\n",
    "- GRPO achieves similar results without a critic by using group-relative advantages.\n",
    "- The Bradley-Terry model provides a principled way to learn rewards from human preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
