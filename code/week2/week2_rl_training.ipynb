{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Week 2: RL Training Methods for Language Models\n\n**MSc Course -- Generative Models in Finance**\n\nThis notebook walks through the key training paradigms used to align and improve language models after pretraining:\n\n| Section | Method | Key Idea |\n|---------|--------|----------|\n| 1 | **SFT** | Supervised fine-tuning on instruction-response pairs |\n| 2 | **REINFORCE** | Classic policy-gradient with a simple reward heuristic |\n| 3 | **PPO** | Proximal Policy Optimisation with GAE and clipped objective |\n| 4 | **GRPO** | Group Relative Policy Optimisation (critic-free) |\n| 5 | **DPO** | Direct Preference Optimisation from pairwise preferences |\n| 6 | **Bradley-Terry Reward Model** | Learn a reward model from preference data |\n| 7 | **GRPO + Lean Verification** | RL for math with a formal theorem prover as reward |\n\nAll models are tiny (~1M parameters) and train on CPU in under 2 minutes.\n\n**Dependencies:** `torch`, `numpy`, `matplotlib`, `requests`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Infrastructure: Tiny GPT-2 Model and Tokeniser\n",
    "\n",
    "We build a minimal GPT-2-style transformer from scratch. It has:\n",
    "- A small vocabulary (character-level + special tokens)\n",
    "- 4 transformer layers, 4 attention heads, embedding dim 128\n",
    "- ~1M parameters\n",
    "\n",
    "This is shared across all experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Simple character-level tokeniser\n",
    "# ---------------------------------------------------------------------------\n",
    "class CharTokeniser:\n",
    "    \"\"\"Character-level tokeniser with special tokens.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Printable ASCII 32-126 plus special tokens\n",
    "        chars = [chr(i) for i in range(32, 127)]  # 95 chars\n",
    "        self.special = {\"<pad>\": 0, \"<bos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "        self.char2id = {**self.special}\n",
    "        for i, c in enumerate(chars):\n",
    "            self.char2id[c] = len(self.special) + i\n",
    "        self.id2char = {v: k for k, v in self.char2id.items()}\n",
    "        self.vocab_size = len(self.char2id)\n",
    "        self.pad_id = self.special[\"<pad>\"]\n",
    "        self.bos_id = self.special[\"<bos>\"]\n",
    "        self.eos_id = self.special[\"<eos>\"]\n",
    "\n",
    "    def encode(self, text: str, add_bos=True, add_eos=True) -> list[int]:\n",
    "        ids = []\n",
    "        if add_bos:\n",
    "            ids.append(self.bos_id)\n",
    "        for c in text:\n",
    "            ids.append(self.char2id.get(c, self.special[\"<unk>\"]))\n",
    "        if add_eos:\n",
    "            ids.append(self.eos_id)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        chars = []\n",
    "        for i in ids:\n",
    "            tok = self.id2char.get(i, \"\")\n",
    "            if tok in (\"<pad>\", \"<bos>\", \"<eos>\", \"<unk>\"):\n",
    "                continue\n",
    "            chars.append(tok)\n",
    "        return \"\".join(chars)\n",
    "\n",
    "\n",
    "tokeniser = CharTokeniser()\n",
    "VOCAB_SIZE = tokeniser.vocab_size\n",
    "print(f\"Vocab size: {VOCAB_SIZE}\")\n",
    "print(f\"Example encode: {tokeniser.encode('hello')}\")\n",
    "print(f\"Example decode: {tokeniser.decode(tokeniser.encode('hello'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Tiny GPT-2 Model\n",
    "# ---------------------------------------------------------------------------\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Causal mask\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(max_len, max_len)).unsqueeze(0).unsqueeze(0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x)  # (B, T, 3C)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        # Reshape to (B, n_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        # Attention scores\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        out = att @ v  # (B, n_heads, T, head_dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.proj(out)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, max_len: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, max_len, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyGPT2(nn.Module):\n",
    "    \"\"\"Minimal GPT-2 style autoregressive language model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 4,\n",
    "        max_len: int = 128,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, n_heads, max_len, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        # Weight tying\n",
    "        self.head.weight = self.tok_emb.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns logits of shape (B, T, vocab_size).\"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.max_len, f\"Sequence length {T} > max_len {self.max_len}\"\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)  # (1, T)\n",
    "        x = self.drop(self.tok_emb(idx) + self.pos_emb(pos))\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Autoregressive generation via sampling.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_len :]\n",
    "            logits = self(idx_cond)[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_tok = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, next_tok], dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Instantiate and inspect\n",
    "_test_model = TinyGPT2(VOCAB_SIZE)\n",
    "print(f\"TinyGPT2 parameters: {count_parameters(_test_model):,}\")\n",
    "del _test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Supervised Fine-Tuning (SFT)\n",
    "\n",
    "SFT is the first step after pretraining. We take a set of **(instruction, response)** pairs and\n",
    "fine-tune the model with **teacher-forced cross-entropy loss**: at each position $t$ the model\n",
    "predicts the next token given the ground-truth prefix.\n",
    "\n",
    "$$\\mathcal{L}_{\\text{SFT}} = -\\frac{1}{T}\\sum_{t=1}^{T} \\log p_\\theta(y_t \\mid y_{<t})$$\n",
    "\n",
    "We train on a small synthetic dataset of instruction-response pairs relevant to finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1a. SFT Dataset\n",
    "# ---------------------------------------------------------------------------\n",
    "SFT_DATA = [\n",
    "    (\"What is a bond?\", \"A bond is a fixed-income instrument representing a loan made by an investor to a borrower.\"),\n",
    "    (\"Define stock.\", \"A stock is a security that represents ownership of a fraction of a corporation.\"),\n",
    "    (\"What is volatility?\", \"Volatility measures the degree of variation in a trading price over time.\"),\n",
    "    (\"Explain diversification.\", \"Diversification is a risk management strategy mixing a variety of investments.\"),\n",
    "    (\"What is a derivative?\", \"A derivative is a financial contract whose value depends on an underlying asset.\"),\n",
    "    (\"Define yield.\", \"Yield is the income return on an investment, typically expressed as a percentage.\"),\n",
    "    (\"What is liquidity?\", \"Liquidity refers to how quickly an asset can be converted to cash without loss.\"),\n",
    "    (\"Explain hedging.\", \"Hedging is an investment to reduce the risk of adverse price movements in an asset.\"),\n",
    "    (\"What is a put option?\", \"A put option gives the holder the right to sell an asset at a specified price.\"),\n",
    "    (\"Define alpha.\", \"Alpha is the excess return of an investment relative to its benchmark index.\"),\n",
    "    (\"What is a call option?\", \"A call option gives the holder the right to buy an asset at a specified price.\"),\n",
    "    (\"Explain leverage.\", \"Leverage is the use of borrowed capital to increase the potential return on investment.\"),\n",
    "    (\"What is beta?\", \"Beta measures the sensitivity of an asset's returns to the overall market returns.\"),\n",
    "    (\"Define portfolio.\", \"A portfolio is a collection of financial investments like stocks, bonds, and cash.\"),\n",
    "    (\"What is arbitrage?\", \"Arbitrage is the simultaneous purchase and sale of an asset to profit from price differences.\"),\n",
    "    (\"Explain short selling.\", \"Short selling involves borrowing shares and selling them, hoping to buy back at lower price.\"),\n",
    "]\n",
    "\n",
    "\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data, tokeniser, max_len=128):\n",
    "        self.samples = []\n",
    "        for instruction, response in data:\n",
    "            text = f\"Q: {instruction} A: {response}\"\n",
    "            ids = tokeniser.encode(text, add_bos=True, add_eos=True)\n",
    "            ids = ids[:max_len]  # truncate\n",
    "            self.samples.append(torch.tensor(ids, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_pad(batch, pad_id=0):\n",
    "    max_len = max(len(s) for s in batch)\n",
    "    padded = torch.full((len(batch), max_len), pad_id, dtype=torch.long)\n",
    "    for i, s in enumerate(batch):\n",
    "        padded[i, : len(s)] = s\n",
    "    return padded\n",
    "\n",
    "\n",
    "sft_dataset = SFTDataset(SFT_DATA, tokeniser)\n",
    "sft_loader = DataLoader(sft_dataset, batch_size=8, shuffle=True, collate_fn=collate_pad)\n",
    "print(f\"SFT dataset size: {len(sft_dataset)} samples\")\n",
    "print(f\"Example (decoded): {tokeniser.decode(sft_dataset[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1b. SFT Training Loop\n",
    "# ---------------------------------------------------------------------------\n",
    "sft_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "sft_optimiser = torch.optim.AdamW(sft_model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "NUM_SFT_EPOCHS = 60\n",
    "sft_losses = []\n",
    "\n",
    "sft_model.train()\n",
    "for epoch in range(NUM_SFT_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for batch in sft_loader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        # Teacher forcing: input = tokens[:-1], target = tokens[1:]\n",
    "        inputs = batch[:, :-1]\n",
    "        targets = batch[:, 1:]\n",
    "        logits = sft_model(inputs)  # (B, T-1, V)\n",
    "        # Flatten for cross-entropy; ignore padding (id=0)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, VOCAB_SIZE),\n",
    "            targets.reshape(-1),\n",
    "            ignore_index=tokeniser.pad_id,\n",
    "        )\n",
    "        sft_optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(sft_model.parameters(), 1.0)\n",
    "        sft_optimiser.step()\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    sft_losses.append(avg_loss)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{NUM_SFT_EPOCHS}  loss={avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nSFT training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1c. SFT Training Loss Curve and Sample Generation\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3.5))\n",
    "ax.plot(sft_losses, linewidth=1.5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Cross-Entropy Loss\")\n",
    "ax.set_title(\"SFT Training Loss\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate a sample\n",
    "sft_model.eval()\n",
    "prompt = \"Q: What is a bond? A:\"\n",
    "prompt_ids = torch.tensor([tokeniser.encode(prompt, add_bos=True, add_eos=False)], device=DEVICE)\n",
    "gen_ids = sft_model.generate(prompt_ids, max_new_tokens=60, temperature=0.7)\n",
    "print(f\"Prompt:    {prompt}\")\n",
    "print(f\"Generated: {tokeniser.decode(gen_ids[0].tolist())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. REINFORCE (Policy Gradient)\n",
    "\n",
    "REINFORCE treats the language model as a **policy** $\\pi_\\theta$ that generates token sequences\n",
    "(actions). A scalar **reward** $R$ is assigned to each complete sequence.\n",
    "\n",
    "The policy gradient estimator is:\n",
    "\n",
    "$$\\nabla_\\theta J = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=1}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t) \\cdot (R(\\tau) - b)\\right]$$\n",
    "\n",
    "where $b$ is an optional **baseline** (e.g. the running mean reward) to reduce variance.\n",
    "\n",
    "**Reward heuristic:** We use a simple reward that encourages the model to generate sequences\n",
    "that (a) contain a target word like \"profit\" and (b) have moderate length (penalising too short\n",
    "or too long outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2a. Reward function and rollout generation\n",
    "# ---------------------------------------------------------------------------\n",
    "TARGET_WORD = \"profit\"\n",
    "\n",
    "\n",
    "def compute_reward(text: str) -> float:\n",
    "    \"\"\"Simple heuristic reward for a generated text.\n",
    "    +2.0 if text contains the target word 'profit'\n",
    "    Length penalty: -0.02 * |len - 40|  (prefer ~40 chars)\n",
    "    Small bonus for ending with a period.\n",
    "    \"\"\"\n",
    "    r = 0.0\n",
    "    if TARGET_WORD in text.lower():\n",
    "        r += 2.0\n",
    "    # Length penalty: prefer around 40 characters\n",
    "    r -= 0.02 * abs(len(text) - 40)\n",
    "    # Punctuation bonus\n",
    "    if text.strip().endswith(\".\"):\n",
    "        r += 0.5\n",
    "    return r\n",
    "\n",
    "\n",
    "def generate_with_logprobs(\n",
    "    model: TinyGPT2,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Generate tokens and collect log-probabilities for each sampled token.\n",
    "    Returns (generated_ids [B, T+max_new], log_probs [B, max_new]).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_log_probs = []\n",
    "    idx = prompt_ids.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.max_len :]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        next_tok = dist.sample()\n",
    "        all_log_probs.append(dist.log_prob(next_tok))\n",
    "        idx = torch.cat([idx, next_tok.unsqueeze(1)], dim=1)\n",
    "    log_probs = torch.stack(all_log_probs, dim=1)  # (B, max_new_tokens)\n",
    "    return idx, log_probs\n",
    "\n",
    "\n",
    "# Quick test\n",
    "rl_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "# Copy SFT weights as starting point\n",
    "rl_model.load_state_dict(sft_model.state_dict())\n",
    "_p = torch.tensor([tokeniser.encode(\"Q: Explain profit. A:\", add_bos=True, add_eos=False)], device=DEVICE)\n",
    "_gen, _lp = generate_with_logprobs(rl_model, _p, max_new_tokens=20)\n",
    "print(f\"Generated: {tokeniser.decode(_gen[0].tolist())}\")\n",
    "print(f\"Log-prob shape: {_lp.shape}, sum: {_lp.sum().item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2b. REINFORCE training (no baseline)\n",
    "# ---------------------------------------------------------------------------\n",
    "def reinforce_step(\n",
    "    model: TinyGPT2,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    batch_size: int,\n",
    "    temperature: float = 1.0,\n",
    "    baseline: float = 0.0,\n",
    ") -> tuple[float, float, list[float]]:\n",
    "    \"\"\"One REINFORCE gradient step.\n",
    "    Returns (mean_reward, policy_loss, list_of_per_sample_grads_norms).\n",
    "    \"\"\"\n",
    "    # Expand prompt for batch\n",
    "    prompts = prompt_ids.expand(batch_size, -1)\n",
    "\n",
    "    # Generate with gradients through log-probs\n",
    "    model.eval()  # keep dropout off for generation\n",
    "    all_log_probs = []\n",
    "    idx = prompts.clone()\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.max_len :]\n",
    "        logits = model(idx_cond)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        next_tok = dist.sample()\n",
    "        all_log_probs.append(dist.log_prob(next_tok))\n",
    "        idx = torch.cat([idx, next_tok.unsqueeze(1).detach()], dim=1)\n",
    "\n",
    "    log_probs = torch.stack(all_log_probs, dim=1)  # (B, max_new_tokens)\n",
    "\n",
    "    # Compute rewards\n",
    "    rewards = []\n",
    "    for i in range(batch_size):\n",
    "        gen_text = tokeniser.decode(idx[i].tolist())\n",
    "        rewards.append(compute_reward(gen_text))\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # REINFORCE loss: -E[log_prob * (R - baseline)]\n",
    "    advantages = rewards_t - baseline\n",
    "    per_sample_loss = -(log_probs.sum(dim=1) * advantages)\n",
    "    loss = per_sample_loss.mean()\n",
    "\n",
    "    return loss, rewards_t.mean().item(), rewards\n",
    "\n",
    "\n",
    "print(\"REINFORCE step function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2c. Train with REINFORCE -- compare with/without baseline\n",
    "# ---------------------------------------------------------------------------\n",
    "def train_reinforce(use_baseline: bool, n_steps: int = 80, batch_size: int = 16, lr: float = 1e-4):\n",
    "    model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "    model.load_state_dict(sft_model.state_dict())\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    prompt = \"Q: Explain profit. A:\"\n",
    "    prompt_ids = torch.tensor(\n",
    "        [tokeniser.encode(prompt, add_bos=True, add_eos=False)], device=DEVICE\n",
    "    )\n",
    "\n",
    "    reward_history = []\n",
    "    grad_norm_history = []\n",
    "    running_baseline = 0.0\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        baseline_val = running_baseline if use_baseline else 0.0\n",
    "        loss, mean_r, _ = reinforce_step(\n",
    "            model, prompt_ids, max_new_tokens=30, batch_size=batch_size,\n",
    "            temperature=1.0, baseline=baseline_val,\n",
    "        )\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        # Record gradient norm\n",
    "        total_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        grad_norm_history.append(total_norm.item())\n",
    "        optimiser.step()\n",
    "\n",
    "        reward_history.append(mean_r)\n",
    "        running_baseline = 0.95 * running_baseline + 0.05 * mean_r\n",
    "\n",
    "        if (step + 1) % 20 == 0:\n",
    "            tag = \"baseline\" if use_baseline else \"no baseline\"\n",
    "            print(f\"  [{tag}] step {step+1:3d}  reward={mean_r:.3f}  grad_norm={total_norm:.3f}\")\n",
    "\n",
    "    return reward_history, grad_norm_history, model\n",
    "\n",
    "\n",
    "print(\"Training REINFORCE without baseline...\")\n",
    "rewards_no_bl, grads_no_bl, _ = train_reinforce(use_baseline=False)\n",
    "print(\"\\nTraining REINFORCE with baseline...\")\n",
    "rewards_bl, grads_bl, reinforce_model = train_reinforce(use_baseline=True)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2d. Visualise REINFORCE results\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Reward curves\n",
    "axes[0].plot(rewards_no_bl, alpha=0.7, label=\"No baseline\")\n",
    "axes[0].plot(rewards_bl, alpha=0.7, label=\"With baseline\")\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Mean Reward\")\n",
    "axes[0].set_title(\"REINFORCE: Reward over Training\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gradient norm comparison\n",
    "axes[1].plot(grads_no_bl, alpha=0.7, label=\"No baseline\")\n",
    "axes[1].plot(grads_bl, alpha=0.7, label=\"With baseline\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Gradient Norm\")\n",
    "axes[1].set_title(\"REINFORCE: Gradient Variance\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "std_no_bl = np.std(grads_no_bl)\n",
    "std_bl = np.std(grads_bl)\n",
    "print(f\"\\nGrad norm std (no baseline):   {std_no_bl:.3f}\")\n",
    "print(f\"Grad norm std (with baseline): {std_bl:.3f}\")\n",
    "reduction = (1 - std_bl / std_no_bl) * 100\n",
    "print(f\"-> The baseline reduces gradient norm std by {reduction:.0f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2e. Show sample generations from the REINFORCE-trained model\n",
    "# ---------------------------------------------------------------------------\n",
    "reinforce_model.eval()\n",
    "prompt = \"Q: Explain profit. A:\"\n",
    "prompt_ids = torch.tensor(\n",
    "    [tokeniser.encode(prompt, add_bos=True, add_eos=False)], device=DEVICE\n",
    ")\n",
    "print(\"Sample generations from REINFORCE model:\\n\")\n",
    "for i in range(5):\n",
    "    gen = reinforce_model.generate(prompt_ids, max_new_tokens=40, temperature=0.8)\n",
    "    text = tokeniser.decode(gen[0].tolist())\n",
    "    r = compute_reward(text)\n",
    "    print(f\"  [{i+1}] (R={r:.2f}) {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. PPO from Scratch\n",
    "\n",
    "Proximal Policy Optimisation (PPO) improves on vanilla REINFORCE with:\n",
    "\n",
    "1. **Clipped surrogate objective** -- prevents the policy from changing too much:\n",
    "   $$L^{\\text{CLIP}} = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta) \\hat{A}_t,\\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "   where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$\n",
    "\n",
    "2. **Generalised Advantage Estimation (GAE)** -- a biased but lower-variance advantage:\n",
    "   $$\\hat{A}_t^{\\text{GAE}} = \\sum_{l=0}^{T-t-1} (\\gamma\\lambda)^l \\delta_{t+l}, \\quad \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "3. **Value function** -- a critic that estimates the expected return from each state.\n",
    "\n",
    "4. **Multiple minibatch epochs** over the same rollout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3a. Value Network (Critic)\n",
    "# ---------------------------------------------------------------------------\n",
    "class ValueHead(nn.Module):\n",
    "    \"\"\"Scalar value head on top of the transformer backbone.\n",
    "    Shares the backbone with the policy but has a separate linear head.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int = 128):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"hidden_states: (B, T, d_model) -> values: (B, T)\"\"\"\n",
    "        return self.head(hidden_states).squeeze(-1)\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Wraps TinyGPT2 (actor) and a ValueHead (critic).\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int = 128, **kwargs):\n",
    "        super().__init__()\n",
    "        self.backbone = TinyGPT2(vocab_size, d_model=d_model, **kwargs)\n",
    "        self.value_head = ValueHead(d_model)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor):\n",
    "        \"\"\"Returns (logits, values).\"\"\"\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.backbone.drop(self.backbone.tok_emb(idx) + self.backbone.pos_emb(pos))\n",
    "        for block in self.backbone.blocks:\n",
    "            x = block(x)\n",
    "        x = self.backbone.ln_f(x)  # (B, T, d_model)\n",
    "        logits = self.backbone.head(x)  # (B, T, V)\n",
    "        values = self.value_head(x)  # (B, T)\n",
    "        return logits, values\n",
    "\n",
    "    def get_logits(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        logits, _ = self(idx)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Quick check\n",
    "_ac = ActorCritic(VOCAB_SIZE)\n",
    "print(f\"ActorCritic parameters: {count_parameters(_ac):,}\")\n",
    "del _ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3b. Rollout collection\n",
    "# ---------------------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def collect_rollouts(\n",
    "    actor_critic: ActorCritic,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    batch_size: int,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    ") -> dict:\n",
    "    \"\"\"Generate sequences and collect data needed for PPO.\"\"\"\n",
    "    prompts = prompt_ids.expand(batch_size, -1)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    idx = prompts.clone()\n",
    "    all_log_probs = []\n",
    "    all_values = []\n",
    "    all_actions = []\n",
    "\n",
    "    for t in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -actor_critic.backbone.max_len :]\n",
    "        logits, values = actor_critic(idx_cond)\n",
    "        # Take last position logits and value\n",
    "        last_logits = logits[:, -1, :] / temperature\n",
    "        last_value = values[:, -1]  # (B,)\n",
    "        probs = F.softmax(last_logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        all_actions.append(action)\n",
    "        all_log_probs.append(log_prob)\n",
    "        all_values.append(last_value)\n",
    "\n",
    "        idx = torch.cat([idx, action.unsqueeze(1)], dim=1)\n",
    "\n",
    "    # Compute terminal values (bootstrap = 0 for complete episodes)\n",
    "    # Compute per-sequence rewards\n",
    "    rewards_list = []\n",
    "    for i in range(batch_size):\n",
    "        gen_text = tokeniser.decode(idx[i].tolist())\n",
    "        rewards_list.append(compute_reward(gen_text))\n",
    "\n",
    "    # We treat the whole generation as one \"step\" with the reward at the end\n",
    "    # For token-level PPO, we assign reward only at the last token\n",
    "    token_rewards = torch.zeros(batch_size, max_new_tokens, device=DEVICE)\n",
    "    for i in range(batch_size):\n",
    "        token_rewards[i, -1] = rewards_list[i]\n",
    "\n",
    "    return {\n",
    "        \"sequences\": idx,  # (B, prompt_len + max_new_tokens)\n",
    "        \"actions\": torch.stack(all_actions, dim=1),  # (B, max_new_tokens)\n",
    "        \"log_probs\": torch.stack(all_log_probs, dim=1),  # (B, max_new_tokens)\n",
    "        \"values\": torch.stack(all_values, dim=1),  # (B, max_new_tokens)\n",
    "        \"token_rewards\": token_rewards,  # (B, max_new_tokens)\n",
    "        \"total_rewards\": torch.tensor(rewards_list, device=DEVICE),  # (B,)\n",
    "        \"prompt_len\": prompt_len,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Rollout collection function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3c. GAE Computation\n",
    "# ---------------------------------------------------------------------------\n",
    "def compute_gae(\n",
    "    token_rewards: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    gamma: float = 1.0,\n",
    "    lam: float = 0.95,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute Generalised Advantage Estimation.\n",
    "\n",
    "    Args:\n",
    "        token_rewards: (B, T) rewards at each token step\n",
    "        values: (B, T) value estimates from critic\n",
    "        gamma: discount factor\n",
    "        lam: GAE lambda parameter\n",
    "\n",
    "    Returns:\n",
    "        advantages: (B, T)\n",
    "        returns: (B, T) = advantages + values\n",
    "    \"\"\"\n",
    "    B, T = token_rewards.shape\n",
    "    advantages = torch.zeros_like(token_rewards)\n",
    "    last_gae = torch.zeros(B, device=token_rewards.device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        if t == T - 1:\n",
    "            next_value = torch.zeros(B, device=token_rewards.device)  # terminal\n",
    "        else:\n",
    "            next_value = values[:, t + 1]\n",
    "        # TD error: delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "        delta = token_rewards[:, t] + gamma * next_value - values[:, t]\n",
    "        # GAE: A_t = delta_t + gamma * lambda * A_{t+1}\n",
    "        last_gae = delta + gamma * lam * last_gae\n",
    "        advantages[:, t] = last_gae\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# Quick test\n",
    "_r = torch.tensor([[0.0, 0.0, 1.0]])\n",
    "_v = torch.tensor([[0.1, 0.2, 0.3]])\n",
    "_adv, _ret = compute_gae(_r, _v, gamma=0.99, lam=0.95)\n",
    "print(f\"Test rewards: {_r}\")\n",
    "print(f\"Test values:  {_v}\")\n",
    "print(f\"GAE advantages: {_adv}\")\n",
    "print(f\"Returns:        {_ret}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3d. PPO Clipped Objective\n",
    "# ---------------------------------------------------------------------------\n",
    "def ppo_loss(\n",
    "    new_log_probs: torch.Tensor,\n",
    "    old_log_probs: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    clip_eps: float = 0.2,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"PPO clipped surrogate objective.\n",
    "\n",
    "    L = min(r_t * A_t, clip(r_t, 1-eps, 1+eps) * A_t)\n",
    "\n",
    "    We negate because we want to maximise the objective (gradient ascent).\n",
    "    \"\"\"\n",
    "    # Importance sampling ratio\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)  # r_t(theta)\n",
    "    # Clipped ratio\n",
    "    clipped_ratio = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "    # Surrogate losses\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = clipped_ratio * advantages\n",
    "    # Take the minimum (pessimistic bound) and negate for minimisation\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    return policy_loss\n",
    "\n",
    "\n",
    "def value_loss(predicted_values: torch.Tensor, returns: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Simple MSE loss for the value function.\"\"\"\n",
    "    return F.mse_loss(predicted_values, returns)\n",
    "\n",
    "\n",
    "print(\"PPO loss functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3e. PPO update step with minibatch epochs\n",
    "# ---------------------------------------------------------------------------\n",
    "def ppo_update(\n",
    "    actor_critic: ActorCritic,\n",
    "    optimiser: torch.optim.Optimizer,\n",
    "    rollout: dict,\n",
    "    n_epochs: int = 4,\n",
    "    minibatch_size: int = 8,\n",
    "    clip_eps: float = 0.2,\n",
    "    vf_coef: float = 0.5,\n",
    "    entropy_coef: float = 0.01,\n",
    "    gamma: float = 1.0,\n",
    "    lam: float = 0.95,\n",
    ") -> dict:\n",
    "    \"\"\"Perform PPO update over multiple epochs of minibatches.\"\"\"\n",
    "    sequences = rollout[\"sequences\"]\n",
    "    actions = rollout[\"actions\"]\n",
    "    old_log_probs = rollout[\"log_probs\"]\n",
    "    old_values = rollout[\"values\"]\n",
    "    token_rewards = rollout[\"token_rewards\"]\n",
    "    prompt_len = rollout[\"prompt_len\"]\n",
    "\n",
    "    # Compute GAE\n",
    "    advantages, returns = compute_gae(token_rewards, old_values, gamma=gamma, lam=lam)\n",
    "    # Normalise advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    B = sequences.shape[0]\n",
    "    T_gen = actions.shape[1]\n",
    "    total_policy_loss = 0.0\n",
    "    total_value_loss = 0.0\n",
    "    n_updates = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle indices\n",
    "        perm = torch.randperm(B)\n",
    "        for start in range(0, B, minibatch_size):\n",
    "            mb_idx = perm[start : start + minibatch_size]\n",
    "            mb_seq = sequences[mb_idx]  # (mb, total_len)\n",
    "            mb_actions = actions[mb_idx]  # (mb, T_gen)\n",
    "            mb_old_lp = old_log_probs[mb_idx]\n",
    "            mb_advantages = advantages[mb_idx]\n",
    "            mb_returns = returns[mb_idx]\n",
    "\n",
    "            # Forward pass through actor-critic\n",
    "            # We feed the full sequence and extract logits/values for the generated part\n",
    "            full_logits, full_values = actor_critic(mb_seq)\n",
    "            # Logits at positions [prompt_len-1 ... prompt_len+T_gen-2] predict tokens\n",
    "            # at positions [prompt_len ... prompt_len+T_gen-1]\n",
    "            gen_logits = full_logits[:, prompt_len - 1 : prompt_len - 1 + T_gen, :]\n",
    "            gen_values = full_values[:, prompt_len - 1 : prompt_len - 1 + T_gen]\n",
    "\n",
    "            # Compute new log probs\n",
    "            gen_log_probs_all = F.log_softmax(gen_logits, dim=-1)\n",
    "            new_log_probs = gen_log_probs_all.gather(\n",
    "                2, mb_actions.unsqueeze(-1)\n",
    "            ).squeeze(-1)\n",
    "\n",
    "            # Entropy bonus\n",
    "            entropy = -(F.softmax(gen_logits, dim=-1) * gen_log_probs_all).sum(-1).mean()\n",
    "\n",
    "            # Losses\n",
    "            p_loss = ppo_loss(new_log_probs, mb_old_lp, mb_advantages, clip_eps=clip_eps)\n",
    "            v_loss = value_loss(gen_values, mb_returns)\n",
    "            total_loss = p_loss + vf_coef * v_loss - entropy_coef * entropy\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(actor_critic.parameters(), 1.0)\n",
    "            optimiser.step()\n",
    "\n",
    "            total_policy_loss += p_loss.item()\n",
    "            total_value_loss += v_loss.item()\n",
    "            n_updates += 1\n",
    "\n",
    "    return {\n",
    "        \"policy_loss\": total_policy_loss / max(n_updates, 1),\n",
    "        \"value_loss\": total_value_loss / max(n_updates, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"PPO update function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3f. PPO Training Loop\n",
    "# ---------------------------------------------------------------------------\n",
    "ppo_ac = ActorCritic(VOCAB_SIZE).to(DEVICE)\n",
    "# Initialise from SFT weights for the backbone\n",
    "ppo_ac.backbone.load_state_dict(sft_model.state_dict())\n",
    "\n",
    "ppo_optimiser = torch.optim.Adam(ppo_ac.parameters(), lr=5e-5)\n",
    "\n",
    "prompt_text = \"Q: Explain profit. A:\"\n",
    "prompt_ids = torch.tensor(\n",
    "    [tokeniser.encode(prompt_text, add_bos=True, add_eos=False)], device=DEVICE\n",
    ")\n",
    "\n",
    "PPO_STEPS = 80\n",
    "ROLLOUT_BATCH = 32\n",
    "GEN_LEN = 30\n",
    "\n",
    "ppo_reward_history = []\n",
    "ppo_policy_loss_history = []\n",
    "ppo_value_loss_history = []\n",
    "\n",
    "print(\"Starting PPO training...\\n\")\n",
    "for step in range(PPO_STEPS):\n",
    "    # 1. Collect rollouts\n",
    "    rollout = collect_rollouts(\n",
    "        ppo_ac, prompt_ids, batch_size=ROLLOUT_BATCH,\n",
    "        max_new_tokens=GEN_LEN, temperature=1.0,\n",
    "    )\n",
    "    mean_reward = rollout[\"total_rewards\"].mean().item()\n",
    "    ppo_reward_history.append(mean_reward)\n",
    "\n",
    "    # 2. PPO update\n",
    "    losses = ppo_update(\n",
    "        ppo_ac, ppo_optimiser, rollout,\n",
    "        n_epochs=4, minibatch_size=8, clip_eps=0.2,\n",
    "        vf_coef=0.5, entropy_coef=0.01,\n",
    "    )\n",
    "    ppo_policy_loss_history.append(losses[\"policy_loss\"])\n",
    "    ppo_value_loss_history.append(losses[\"value_loss\"])\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Step {step+1:3d}/{PPO_STEPS}  \"\n",
    "            f\"reward={mean_reward:.3f}  \"\n",
    "            f\"pi_loss={losses['policy_loss']:.4f}  \"\n",
    "            f\"v_loss={losses['value_loss']:.4f}\"\n",
    "        )\n",
    "\n",
    "print(\"\\nPPO training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3g. PPO Training Plots\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(ppo_reward_history, linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Mean Reward\")\n",
    "axes[0].set_title(\"PPO: Reward over Training\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ppo_policy_loss_history, linewidth=1.5, color=\"tab:orange\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Policy Loss\")\n",
    "axes[1].set_title(\"PPO: Clipped Policy Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(ppo_value_loss_history, linewidth=1.5, color=\"tab:green\")\n",
    "axes[2].set_xlabel(\"Step\")\n",
    "axes[2].set_ylabel(\"Value Loss\")\n",
    "axes[2].set_title(\"PPO: Value Function Loss\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3h. PPO Sample Generations\n",
    "# ---------------------------------------------------------------------------\n",
    "ppo_ac.eval()\n",
    "print(\"Sample generations from PPO-trained model:\\n\")\n",
    "for i in range(5):\n",
    "    with torch.no_grad():\n",
    "        gen = ppo_ac.backbone.generate(prompt_ids, max_new_tokens=40, temperature=0.8)\n",
    "    text = tokeniser.decode(gen[0].tolist())\n",
    "    r = compute_reward(text)\n",
    "    print(f\"  [{i+1}] (R={r:.2f}) {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Group Relative Policy Optimisation (GRPO)\n",
    "\n",
    "GRPO (from DeepSeek-R1) is a **critic-free** alternative to PPO. For each prompt:\n",
    "\n",
    "1. Generate a **group** of $G$ responses from the current policy.\n",
    "2. Score each response with a reward function.\n",
    "3. Compute **group-normalised advantages**: $A_i = \\frac{R_i - \\mu_G}{\\sigma_G}$\n",
    "4. Update the policy with a clipped objective (like PPO) but using these advantages.\n",
    "\n",
    "Key benefit: no value network needed, reducing complexity and training instability.\n",
    "\n",
    "Additionally, GRPO includes a KL penalty to keep the policy close to a reference:\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_i|}\\sum_{t=1}^{|o_i|}\\left[\\min(r_t A_i,\\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) A_i) - \\beta\\, D_{KL}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4a. GRPO: Generate group and compute group-normalised advantages\n",
    "# ---------------------------------------------------------------------------\n",
    "def grpo_generate_group(\n",
    "    model: TinyGPT2,\n",
    "    prompt_ids: torch.Tensor,\n",
    "    group_size: int,\n",
    "    max_new_tokens: int,\n",
    "    temperature: float = 1.0,\n",
    ") -> dict:\n",
    "    \"\"\"Generate a group of responses and compute group-normalised advantages.\"\"\"\n",
    "    prompts = prompt_ids.expand(group_size, -1)\n",
    "    prompt_len = prompt_ids.shape[1]\n",
    "\n",
    "    # Generate with log-probs (need gradients for update)\n",
    "    model.eval()\n",
    "    idx = prompts.clone()\n",
    "    all_log_probs = []\n",
    "    all_actions = []\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -model.max_len :]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)[:, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        all_actions.append(action)\n",
    "        all_log_probs.append(log_prob)\n",
    "        idx = torch.cat([idx, action.unsqueeze(1)], dim=1)\n",
    "\n",
    "    # Compute rewards\n",
    "    rewards = []\n",
    "    for i in range(group_size):\n",
    "        text = tokeniser.decode(idx[i].tolist())\n",
    "        rewards.append(compute_reward(text))\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    # Group-normalised advantages: A_i = (R_i - mean) / std\n",
    "    mean_r = rewards_t.mean()\n",
    "    std_r = rewards_t.std() + 1e-8\n",
    "    advantages = (rewards_t - mean_r) / std_r  # (G,)\n",
    "\n",
    "    return {\n",
    "        \"sequences\": idx,\n",
    "        \"actions\": torch.stack(all_actions, dim=1),\n",
    "        \"old_log_probs\": torch.stack(all_log_probs, dim=1),\n",
    "        \"advantages\": advantages,\n",
    "        \"rewards\": rewards_t,\n",
    "        \"prompt_len\": prompt_len,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"GRPO group generation function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4b. GRPO Update Step\n",
    "# ---------------------------------------------------------------------------\n",
    "def grpo_update(\n",
    "    model: TinyGPT2,\n",
    "    ref_model: TinyGPT2,\n",
    "    optimiser: torch.optim.Optimizer,\n",
    "    group_data: dict,\n",
    "    clip_eps: float = 0.2,\n",
    "    beta_kl: float = 0.04,\n",
    "    n_epochs: int = 2,\n",
    ") -> float:\n",
    "    \"\"\"GRPO policy update with clipped objective and KL penalty.\"\"\"\n",
    "    sequences = group_data[\"sequences\"]\n",
    "    actions = group_data[\"actions\"]\n",
    "    old_log_probs = group_data[\"old_log_probs\"]\n",
    "    advantages = group_data[\"advantages\"]  # (G,)\n",
    "    prompt_len = group_data[\"prompt_len\"]\n",
    "    G, T_gen = actions.shape\n",
    "\n",
    "    total_loss_val = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        # Forward pass\n",
    "        logits = model(sequences)  # (G, total_len, V)\n",
    "        gen_logits = logits[:, prompt_len - 1 : prompt_len - 1 + T_gen, :]\n",
    "        gen_log_probs = F.log_softmax(gen_logits, dim=-1)\n",
    "        new_log_probs = gen_log_probs.gather(2, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Reference model log probs (for KL)\n",
    "        with torch.no_grad():\n",
    "            ref_logits = ref_model(sequences)\n",
    "            ref_gen_logits = ref_logits[:, prompt_len - 1 : prompt_len - 1 + T_gen, :]\n",
    "            ref_log_probs = F.log_softmax(ref_gen_logits, dim=-1)\n",
    "            ref_lp = ref_log_probs.gather(2, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Importance sampling ratio\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)  # (G, T_gen)\n",
    "        clipped_ratio = torch.clamp(ratio, 1.0 - clip_eps, 1.0 + clip_eps)\n",
    "\n",
    "        # Expand advantages to token level: (G,) -> (G, T_gen)\n",
    "        adv_expanded = advantages.unsqueeze(1).expand_as(ratio)\n",
    "\n",
    "        surr1 = ratio * adv_expanded\n",
    "        surr2 = clipped_ratio * adv_expanded\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # KL divergence penalty (approx): D_KL = exp(ref_lp - new_lp) - (ref_lp - new_lp) - 1\n",
    "        log_ratio_kl = ref_lp - new_log_probs\n",
    "        kl = (torch.exp(log_ratio_kl) - log_ratio_kl - 1.0).mean()\n",
    "\n",
    "        loss = policy_loss + beta_kl * kl\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimiser.step()\n",
    "        total_loss_val += loss.item()\n",
    "\n",
    "    return total_loss_val / n_epochs\n",
    "\n",
    "\n",
    "print(\"GRPO update function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4c. GRPO Training Loop\n",
    "# ---------------------------------------------------------------------------\n",
    "grpo_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "grpo_model.load_state_dict(sft_model.state_dict())\n",
    "grpo_ref = copy.deepcopy(grpo_model)  # frozen reference\n",
    "grpo_ref.eval()\n",
    "for p in grpo_ref.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "grpo_optimiser = torch.optim.Adam(grpo_model.parameters(), lr=5e-5)\n",
    "\n",
    "GRPO_STEPS = 80\n",
    "GROUP_SIZE = 32\n",
    "\n",
    "grpo_reward_history = []\n",
    "\n",
    "print(\"Starting GRPO training...\\n\")\n",
    "for step in range(GRPO_STEPS):\n",
    "    group_data = grpo_generate_group(\n",
    "        grpo_model, prompt_ids, group_size=GROUP_SIZE,\n",
    "        max_new_tokens=GEN_LEN, temperature=1.0,\n",
    "    )\n",
    "    mean_r = group_data[\"rewards\"].mean().item()\n",
    "    grpo_reward_history.append(mean_r)\n",
    "\n",
    "    loss = grpo_update(\n",
    "        grpo_model, grpo_ref, grpo_optimiser, group_data,\n",
    "        clip_eps=0.2, beta_kl=0.04, n_epochs=2,\n",
    "    )\n",
    "\n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step {step+1:3d}/{GRPO_STEPS}  reward={mean_r:.3f}  loss={loss:.4f}\")\n",
    "\n",
    "print(\"\\nGRPO training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4d. Compare PPO vs GRPO\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(ppo_reward_history, label=\"PPO\", linewidth=1.5)\n",
    "ax.plot(grpo_reward_history, label=\"GRPO\", linewidth=1.5)\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Mean Reward\")\n",
    "ax.set_title(\"PPO vs GRPO: Reward over Training\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PPO final reward (last 10 avg):  {np.mean(ppo_reward_history[-10:]):.3f}\")\n",
    "print(f\"GRPO final reward (last 10 avg): {np.mean(grpo_reward_history[-10:]):.3f}\")\n",
    "print(\"\\nGRPO achieves comparable rewards without needing a critic network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Direct Preference Optimisation (DPO)\n",
    "\n",
    "DPO eliminates the need for a separate reward model entirely. Given **preference pairs**\n",
    "$(y_w, y_l)$ where $y_w$ is preferred over $y_l$, DPO directly optimises the policy:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}} = -\\mathbb{E}\\left[\\log\\sigma\\left(\\beta\\left(\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{\\text{ref}}(y_w|x)} - \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{\\text{ref}}(y_l|x)}\\right)\\right)\\right]$$\n",
    "\n",
    "The implicit reward under DPO is:\n",
    "$$r(x, y) = \\beta \\log\\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta\\log Z(x)$$\n",
    "\n",
    "We construct synthetic preference pairs from our finance QA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5a. Synthetic Preference Dataset\n",
    "# ---------------------------------------------------------------------------\n",
    "# Each tuple: (prompt, preferred_response, dispreferred_response)\n",
    "DPO_PREFS = [\n",
    "    (\"What is a bond?\",\n",
    "     \"A bond is a fixed-income instrument representing a loan from investor to borrower.\",\n",
    "     \"A bond is some kind of financial thing.\"),\n",
    "    (\"Define stock.\",\n",
    "     \"A stock represents ownership of a fraction of a corporation.\",\n",
    "     \"Stock is stuff you buy.\"),\n",
    "    (\"What is volatility?\",\n",
    "     \"Volatility measures the degree of variation in trading price over time.\",\n",
    "     \"Volatility is when prices go up and down.\"),\n",
    "    (\"Explain diversification.\",\n",
    "     \"Diversification is a risk management strategy mixing various investments.\",\n",
    "     \"Diversification means buying different things.\"),\n",
    "    (\"What is a derivative?\",\n",
    "     \"A derivative is a financial contract whose value depends on an underlying asset.\",\n",
    "     \"A derivative is complicated finance stuff.\"),\n",
    "    (\"Define yield.\",\n",
    "     \"Yield is the income return on an investment expressed as a percentage.\",\n",
    "     \"Yield is the money you get.\"),\n",
    "    (\"What is liquidity?\",\n",
    "     \"Liquidity refers to how quickly an asset converts to cash without loss.\",\n",
    "     \"Liquidity is about cash.\"),\n",
    "    (\"Explain hedging.\",\n",
    "     \"Hedging is an investment to reduce the risk of adverse price movements.\",\n",
    "     \"Hedging is like insurance or something.\"),\n",
    "    (\"What is leverage?\",\n",
    "     \"Leverage is using borrowed capital to increase potential return on investment.\",\n",
    "     \"Leverage means borrowing money.\"),\n",
    "    (\"Define alpha.\",\n",
    "     \"Alpha is the excess return of an investment relative to its benchmark index.\",\n",
    "     \"Alpha is a greek letter used in finance.\"),\n",
    "    (\"What is arbitrage?\",\n",
    "     \"Arbitrage is the simultaneous purchase and sale of an asset to profit from price differences.\",\n",
    "     \"Arbitrage is free money in the market.\"),\n",
    "    (\"Explain short selling.\",\n",
    "     \"Short selling involves borrowing shares and selling them, hoping to buy back at a lower price.\",\n",
    "     \"Short selling is betting stocks go down.\"),\n",
    "]\n",
    "\n",
    "\n",
    "def compute_sequence_log_prob(\n",
    "    model: TinyGPT2, full_ids: torch.Tensor, prompt_len: int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute sum of log-probs for the response portion of a sequence.\n",
    "    full_ids: (1, total_len), prompt_len: length of the prompt prefix.\n",
    "    Returns scalar log-prob.\n",
    "    \"\"\"\n",
    "    logits = model(full_ids)  # (1, total_len, V)\n",
    "    # Shift: logits[t] predicts token[t+1]\n",
    "    # Response tokens are at positions [prompt_len ... total_len-1]\n",
    "    # Corresponding logits are at positions [prompt_len-1 ... total_len-2]\n",
    "    response_logits = logits[:, prompt_len - 1 : -1, :]  # (1, resp_len, V)\n",
    "    response_targets = full_ids[:, prompt_len:]  # (1, resp_len)\n",
    "    log_probs = F.log_softmax(response_logits, dim=-1)\n",
    "    token_lp = log_probs.gather(2, response_targets.unsqueeze(-1)).squeeze(-1)\n",
    "    return token_lp.sum(dim=1)  # (1,)\n",
    "\n",
    "\n",
    "print(f\"DPO preference pairs: {len(DPO_PREFS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5b. DPO Training\n",
    "# ---------------------------------------------------------------------------\n",
    "dpo_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\n",
    "dpo_model.load_state_dict(sft_model.state_dict())\n",
    "dpo_ref = copy.deepcopy(dpo_model)\n",
    "dpo_ref.eval()\n",
    "for p in dpo_ref.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "dpo_optimiser = torch.optim.Adam(dpo_model.parameters(), lr=1e-4)\n",
    "\n",
    "DPO_BETA = 0.1\n",
    "DPO_EPOCHS = 40\n",
    "\n",
    "dpo_losses = []\n",
    "implicit_reward_margins = []  # track reward_w - reward_l over training\n",
    "\n",
    "print(\"Starting DPO training...\\n\")\n",
    "for epoch in range(DPO_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_margins = []\n",
    "\n",
    "    # Shuffle preference pairs\n",
    "    perm = np.random.permutation(len(DPO_PREFS))\n",
    "\n",
    "    for idx in perm:\n",
    "        prompt, y_w, y_l = DPO_PREFS[idx]\n",
    "        prompt_text = f\"Q: {prompt} A: \"\n",
    "\n",
    "        # Encode full sequences\n",
    "        prompt_ids_enc = tokeniser.encode(prompt_text, add_bos=True, add_eos=False)\n",
    "        prompt_len = len(prompt_ids_enc)\n",
    "\n",
    "        yw_ids = prompt_ids_enc + tokeniser.encode(y_w, add_bos=False, add_eos=True)\n",
    "        yl_ids = prompt_ids_enc + tokeniser.encode(y_l, add_bos=False, add_eos=True)\n",
    "\n",
    "        yw_t = torch.tensor([yw_ids], dtype=torch.long, device=DEVICE)\n",
    "        yl_t = torch.tensor([yl_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        # Current policy log probs\n",
    "        lp_w = compute_sequence_log_prob(dpo_model, yw_t, prompt_len)\n",
    "        lp_l = compute_sequence_log_prob(dpo_model, yl_t, prompt_len)\n",
    "\n",
    "        # Reference policy log probs\n",
    "        with torch.no_grad():\n",
    "            ref_lp_w = compute_sequence_log_prob(dpo_ref, yw_t, prompt_len)\n",
    "            ref_lp_l = compute_sequence_log_prob(dpo_ref, yl_t, prompt_len)\n",
    "\n",
    "        # DPO loss: -log sigma(beta * (log pi(yw)/pi_ref(yw) - log pi(yl)/pi_ref(yl)))\n",
    "        log_ratio_w = lp_w - ref_lp_w  # log(pi/pi_ref) for preferred\n",
    "        log_ratio_l = lp_l - ref_lp_l  # log(pi/pi_ref) for dispreferred\n",
    "        logit = DPO_BETA * (log_ratio_w - log_ratio_l)\n",
    "        loss = -F.logsigmoid(logit).mean()\n",
    "\n",
    "        dpo_optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(dpo_model.parameters(), 1.0)\n",
    "        dpo_optimiser.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Track implicit reward margin\n",
    "        with torch.no_grad():\n",
    "            implicit_rw = DPO_BETA * (compute_sequence_log_prob(dpo_model, yw_t, prompt_len) -\n",
    "                                       compute_sequence_log_prob(dpo_ref, yw_t, prompt_len))\n",
    "            implicit_rl = DPO_BETA * (compute_sequence_log_prob(dpo_model, yl_t, prompt_len) -\n",
    "                                       compute_sequence_log_prob(dpo_ref, yl_t, prompt_len))\n",
    "            epoch_margins.append((implicit_rw - implicit_rl).item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(DPO_PREFS)\n",
    "    avg_margin = np.mean(epoch_margins)\n",
    "    dpo_losses.append(avg_loss)\n",
    "    implicit_reward_margins.append(avg_margin)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{DPO_EPOCHS}  loss={avg_loss:.4f}  reward_margin={avg_margin:.4f}\")\n",
    "\n",
    "print(\"\\nDPO training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5c. DPO Visualisation: Loss and Implicit Reward Margin\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(dpo_losses, linewidth=1.5)\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"DPO Loss\")\n",
    "axes[0].set_title(\"DPO: Training Loss\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(implicit_reward_margins, linewidth=1.5, color=\"tab:red\")\n",
    "axes[1].axhline(y=0, color=\"grey\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Implicit Reward Margin (w - l)\")\n",
    "axes[1].set_title(\"DPO: Implicit Reward Margin\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The implicit reward margin increases: the model learns to assign higher\")\n",
    "print(\"implicit reward to preferred responses and lower to dispreferred ones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5d. DPO: Inspect Implicit Rewards for Specific Pairs\n",
    "# ---------------------------------------------------------------------------\n",
    "dpo_model.eval()\n",
    "print(\"Implicit reward for each preference pair:\\n\")\n",
    "print(f\"{'Prompt':<30} {'R(preferred)':>14} {'R(dispreferred)':>16} {'Margin':>8}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for prompt, y_w, y_l in DPO_PREFS[:6]:\n",
    "    prompt_text = f\"Q: {prompt} A: \"\n",
    "    prompt_ids_enc = tokeniser.encode(prompt_text, add_bos=True, add_eos=False)\n",
    "    p_len = len(prompt_ids_enc)\n",
    "\n",
    "    yw_ids = prompt_ids_enc + tokeniser.encode(y_w, add_bos=False, add_eos=True)\n",
    "    yl_ids = prompt_ids_enc + tokeniser.encode(y_l, add_bos=False, add_eos=True)\n",
    "\n",
    "    yw_t = torch.tensor([yw_ids], dtype=torch.long, device=DEVICE)\n",
    "    yl_t = torch.tensor([yl_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        r_w = DPO_BETA * (\n",
    "            compute_sequence_log_prob(dpo_model, yw_t, p_len) -\n",
    "            compute_sequence_log_prob(dpo_ref, yw_t, p_len)\n",
    "        ).item()\n",
    "        r_l = DPO_BETA * (\n",
    "            compute_sequence_log_prob(dpo_model, yl_t, p_len) -\n",
    "            compute_sequence_log_prob(dpo_ref, yl_t, p_len)\n",
    "        ).item()\n",
    "\n",
    "    print(f\"{prompt:<30} {r_w:>14.4f} {r_l:>16.4f} {r_w - r_l:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Bradley-Terry Reward Model\n",
    "\n",
    "The **Bradley-Terry** model is the standard approach for training a reward model from\n",
    "pairwise preference data. Given a pair $(y_w, y_l)$ where $y_w$ is preferred:\n",
    "\n",
    "$$P(y_w \\succ y_l) = \\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))$$\n",
    "\n",
    "The loss is:\n",
    "$$\\mathcal{L}_{\\text{BT}} = -\\mathbb{E}\\left[\\log\\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))\\right]$$\n",
    "\n",
    "We train a small reward model that takes a (prompt, response) pair and outputs a scalar reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6a. Reward Model Architecture\n",
    "# ---------------------------------------------------------------------------\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Small reward model: transformer backbone + scalar head.\n",
    "    Uses a separate (smaller) backbone for efficiency.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int = 96, n_heads: int = 4,\n",
    "                 n_layers: int = 3, max_len: int = 128):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(d_model, n_heads, max_len, dropout=0.1)\n",
    "             for _ in range(n_layers)]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.reward_head = nn.Linear(d_model, 1)\n",
    "        self.max_len = max_len\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns scalar reward for each sequence in the batch. Shape: (B, 1).\"\"\"\n",
    "        B, T = idx.shape\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        # Pool: take the last non-padding token representation\n",
    "        # For simplicity, use the last position\n",
    "        last_hidden = x[:, -1, :]  # (B, d_model)\n",
    "        reward = self.reward_head(last_hidden)  # (B, 1)\n",
    "        return reward\n",
    "\n",
    "\n",
    "rm = RewardModel(VOCAB_SIZE).to(DEVICE)\n",
    "print(f\"Reward Model parameters: {count_parameters(rm):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6b. Bradley-Terry Training\n",
    "# ---------------------------------------------------------------------------\n",
    "# Prepare data: split into train/test\n",
    "all_bt_data = []\n",
    "for prompt, y_w, y_l in DPO_PREFS:\n",
    "    full_w = f\"Q: {prompt} A: {y_w}\"\n",
    "    full_l = f\"Q: {prompt} A: {y_l}\"\n",
    "    ids_w = tokeniser.encode(full_w, add_bos=True, add_eos=True)\n",
    "    ids_l = tokeniser.encode(full_l, add_bos=True, add_eos=True)\n",
    "    all_bt_data.append((ids_w, ids_l))\n",
    "\n",
    "# 9 train, 3 test\n",
    "train_bt = all_bt_data[:9]\n",
    "test_bt = all_bt_data[9:]\n",
    "\n",
    "\n",
    "def pad_pair(ids_w, ids_l, pad_id=0):\n",
    "    max_len = max(len(ids_w), len(ids_l))\n",
    "    w = ids_w + [pad_id] * (max_len - len(ids_w))\n",
    "    l = ids_l + [pad_id] * (max_len - len(ids_l))\n",
    "    return (\n",
    "        torch.tensor([w], dtype=torch.long, device=DEVICE),\n",
    "        torch.tensor([l], dtype=torch.long, device=DEVICE),\n",
    "    )\n",
    "\n",
    "\n",
    "rm_optimiser = torch.optim.Adam(rm.parameters(), lr=1e-3)\n",
    "BT_EPOCHS = 60\n",
    "bt_losses = []\n",
    "\n",
    "print(\"Training Bradley-Terry Reward Model...\\n\")\n",
    "for epoch in range(BT_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    perm = np.random.permutation(len(train_bt))\n",
    "    for i in perm:\n",
    "        ids_w, ids_l = train_bt[i]\n",
    "        tw, tl = pad_pair(ids_w, ids_l)\n",
    "\n",
    "        r_w = rm(tw)  # (1, 1)\n",
    "        r_l = rm(tl)  # (1, 1)\n",
    "\n",
    "        # BT loss: -log sigma(r_w - r_l)\n",
    "        loss = -F.logsigmoid(r_w - r_l).mean()\n",
    "\n",
    "        rm_optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        rm_optimiser.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_bt)\n",
    "    bt_losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{BT_EPOCHS}  loss={avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nBradley-Terry training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6c. Evaluate Reward Model\n",
    "# ---------------------------------------------------------------------------\n",
    "# Training loss plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3.5))\n",
    "ax.plot(bt_losses, linewidth=1.5)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Bradley-Terry Loss\")\n",
    "ax.set_title(\"Reward Model: Training Loss\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Accuracy on train and test sets\n",
    "rm.eval()\n",
    "\n",
    "\n",
    "def eval_accuracy(data, label):\n",
    "    correct = 0\n",
    "    total = len(data)\n",
    "    for ids_w, ids_l in data:\n",
    "        tw, tl = pad_pair(ids_w, ids_l)\n",
    "        with torch.no_grad():\n",
    "            r_w = rm(tw).item()\n",
    "            r_l = rm(tl).item()\n",
    "        if r_w > r_l:\n",
    "            correct += 1\n",
    "    acc = correct / total\n",
    "    print(f\"{label} accuracy: {correct}/{total} = {acc:.1%}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "train_acc = eval_accuracy(train_bt, \"Train\")\n",
    "test_acc = eval_accuracy(test_bt, \"Test\")\n",
    "\n",
    "# Show reward scores for all pairs\n",
    "print(f\"\\n{'Prompt':<30} {'R(preferred)':>13} {'R(dispreferred)':>16} {'Correct?':>9}\")\n",
    "print(\"-\" * 72)\n",
    "for i, (prompt, y_w, y_l) in enumerate(DPO_PREFS):\n",
    "    ids_w, ids_l = all_bt_data[i]\n",
    "    tw, tl = pad_pair(ids_w, ids_l)\n",
    "    with torch.no_grad():\n",
    "        r_w = rm(tw).item()\n",
    "        r_l = rm(tl).item()\n",
    "    correct = \"yes\" if r_w > r_l else \"NO\"\n",
    "    split = \"train\" if i < 9 else \"test\"\n",
    "    print(f\"[{split}] {prompt:<25} {r_w:>12.4f} {r_l:>16.4f} {correct:>9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7. RL for Mathematics with Lean Verification\n\nIn Sections 24 we used a **heuristic reward** (does the response contain \"profit\"?). In practice, the reward model is a learned neural network whose output is subjective and hackable.\n\n**Mathematics is different.** A formal theorem prover such as [Lean 4](https://lean-lang.org/) can verify proofs with absolute certainty. This gives us a **perfect, non-hackable reward signal**:\n\n$$R(x, y) = \\begin{cases} +1 & \\text{if Lean verifies the proof } y \\text{ for theorem } x \\\\ \\phantom{+}0 & \\text{otherwise} \\end{cases}$$\n\nThis section demonstrates the idea on a toy scale:\n1. Define a small set of simple Lean theorems.\n2. Build a reward function that sends generated proofs to a **public Lean server** for verification.\n3. Train a tiny model with **GRPO** (Section 4) using verification as the reward.\n\nThis is the approach used by **DeepSeek-Prover** and **AlphaProof** at scale.\n\n> **Note:** This section requires internet access to reach the Lean server. If the server is unavailable, cached results are used automatically.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# 7a. Lean Verifier Client\n# ---------------------------------------------------------------------------\nimport requests, json as _json, hashlib\n\n# --- Configuration ---\n# Point this to any Lean 4 server that accepts POST requests with a \"cmd\"\n# field and returns JSON with diagnostics.  The default is the public\n# lean4web instance; change it if you run your own (e.g. via lean-lsp-mcp).\nLEAN_SERVER_URL = \"https://live.lean-lang.org/api/check\"\n\n# Cache verified results so we don't spam the server during RL training\n_lean_cache: dict[str, bool] = {}\n\n\ndef verify_lean_code(code: str, timeout: float = 30.0) -> dict:\n    \"\"\"Send Lean 4 code to a public server and return verification result.\n\n    Returns\n    -------\n    dict with keys:\n        verified : bool    True iff no errors\n        messages : list    diagnostic messages from Lean\n        cached   : bool    True if result came from cache\n    \"\"\"\n    cache_key = hashlib.sha256(code.encode()).hexdigest()\n    if cache_key in _lean_cache:\n        return {\"verified\": _lean_cache[cache_key], \"messages\": [], \"cached\": True}\n\n    try:\n        resp = requests.post(\n            LEAN_SERVER_URL,\n            json={\"cmd\": code, \"env\": 0},\n            timeout=timeout,\n        )\n        resp.raise_for_status()\n        data = resp.json()\n\n        # lean4web returns {\"env\": int, \"messages\": [...]}\n        messages = data.get(\"messages\", [])\n        has_error = any(\n            m.get(\"severity\", \"\") == \"error\" for m in messages\n        )\n        verified = not has_error\n    except Exception as e:\n        # Server unreachable  return unknown\n        return {\"verified\": False, \"messages\": [str(e)], \"cached\": False}\n\n    _lean_cache[cache_key] = verified\n    return {\"verified\": verified, \"messages\": messages, \"cached\": False}\n\n\n# Quick smoke test\ntest_result = verify_lean_code(\"theorem test : 2 + 2 = 4 := by norm_num\")\nprint(f\"Smoke test  'theorem test : 2 + 2 = 4 := by norm_num'\")\nprint(f\"  verified: {test_result['verified']}\")\nprint(f\"  cached:   {test_result['cached']}\")\n\nbad_result = verify_lean_code(\"theorem bad : 2 + 2 = 5 := by norm_num\")\nprint(f\"\\nSmoke test  'theorem bad : 2 + 2 = 5 := by norm_num'\")\nprint(f\"  verified: {bad_result['verified']}\")\n\nif test_result[\"verified\"] and not bad_result[\"verified\"]:\n    print(\"\\n Lean server is reachable and working correctly.\")\n    LEAN_AVAILABLE = True\nelse:\n    print(\"\\n Lean server may be unavailable. Using cached fallback.\")\n    LEAN_AVAILABLE = False",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# 7b. Training Data: Simple Lean Theorems\n# ---------------------------------------------------------------------------\n# Each entry: (statement, correct_tactic, description)\n# The model will learn to generate the tactic given the statement.\n\nLEAN_THEOREMS = [\n    {\n        \"statement\": \"theorem t1 : 2 + 3 = 5\",\n        \"tactic\": \"norm_num\",\n        \"desc\": \"simple arithmetic\",\n    },\n    {\n        \"statement\": \"theorem t2 : 10 - 3 = 7\",\n        \"tactic\": \"norm_num\",\n        \"desc\": \"subtraction\",\n    },\n    {\n        \"statement\": \"theorem t3 : 3 * 4 = 12\",\n        \"tactic\": \"norm_num\",\n        \"desc\": \"multiplication\",\n    },\n    {\n        \"statement\": \"theorem t4 : (1 + 1) * 3 = 6\",\n        \"tactic\": \"norm_num\",\n        \"desc\": \"nested arithmetic\",\n    },\n    {\n        \"statement\": \"theorem t5 (a : Nat) : a + 0 = a\",\n        \"tactic\": \"simp\",\n        \"desc\": \"additive identity\",\n    },\n    {\n        \"statement\": \"theorem t6 (a : Nat) : 0 + a = a\",\n        \"tactic\": \"simp\",\n        \"desc\": \"additive identity (comm)\",\n    },\n    {\n        \"statement\": \"theorem t7 (a b : Nat) : a + b = b + a\",\n        \"tactic\": \"omega\",\n        \"desc\": \"commutativity of addition\",\n    },\n    {\n        \"statement\": \"theorem t8 (a b c : Nat) : a + b + c = a + (b + c)\",\n        \"tactic\": \"omega\",\n        \"desc\": \"associativity of addition\",\n    },\n    {\n        \"statement\": \"theorem t9 (n : Nat) : n * 1 = n\",\n        \"tactic\": \"simp\",\n        \"desc\": \"multiplicative identity\",\n    },\n    {\n        \"statement\": \"theorem t10 (n : Nat) : n * 0 = 0\",\n        \"tactic\": \"simp\",\n        \"desc\": \"multiplication by zero\",\n    },\n]\n\n# Build a cache of known correct results (fallback if server is down)\nKNOWN_CORRECT: dict[str, str] = {}\nfor thm in LEAN_THEOREMS:\n    full_code = f\"{thm['statement']} := by {thm['tactic']}\"\n    _lean_cache[hashlib.sha256(full_code.encode()).hexdigest()] = True\n    KNOWN_CORRECT[thm[\"statement\"]] = thm[\"tactic\"]\n\nprint(f\"Defined {len(LEAN_THEOREMS)} training theorems.\")\nprint(\"\\nExamples:\")\nfor t in LEAN_THEOREMS[:3]:\n    print(f\"  {t['statement']} := by {t['tactic']}  ({t['desc']})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# 7c. Lean-based Reward Function\n# ---------------------------------------------------------------------------\n\n# Simple tokeniser for Lean tactic strings: we reuse CharTokeniser but\n# restrict the vocabulary to characters that appear in our tactics.\nLEAN_TACTICS = [\"norm_num\", \"simp\", \"omega\", \"ring\", \"decide\", \"rfl\"]\n\n\ndef compute_lean_reward(theorem_statement: str, generated_tactic: str) -> float:\n    \"\"\"Verify a generated tactic proof via Lean and return a scalar reward.\n\n    Reward scheme:\n        +1.0  if Lean verifies the proof successfully\n        -0.2  if verification fails (encourages exploration without harsh penalty)\n         0.0  if the generated text is empty or unparseable\n    \"\"\"\n    tactic = generated_tactic.strip()\n    if not tactic:\n        return 0.0\n\n    # Construct the full Lean file\n    lean_code = f\"{theorem_statement} := by {tactic}\"\n\n    result = verify_lean_code(lean_code)\n    if result[\"verified\"]:\n        return 1.0\n    else:\n        return -0.2\n\n\n# Test the reward function\nfor thm in LEAN_THEOREMS[:3]:\n    r_correct = compute_lean_reward(thm[\"statement\"], thm[\"tactic\"])\n    r_wrong = compute_lean_reward(thm[\"statement\"], \"sorry\")\n    print(f\"{thm['desc']:30s}  correct={r_correct:+.1f}  wrong={r_wrong:+.1f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# 7d. GRPO Training with Lean Verification Reward\n# ---------------------------------------------------------------------------\n# We use a simplified GRPO loop that generates candidate tactics (as short\n# character strings) for each theorem and updates based on Lean verification.\n#\n# Because our TinyGPT2 is character-level and tactics are short strings,\n# we encode the prompt as: \"<theorem statement> := by \"\n# and let the model generate the tactic completion.\n\n# Prepare prompt tokens for each theorem\nlean_prompts = []\nfor thm in LEAN_THEOREMS:\n    prompt_text = f\"{thm['statement']} := by \"\n    ids = tokeniser.encode(prompt_text)\n    lean_prompts.append(torch.tensor([ids], dtype=torch.long, device=DEVICE))\n\n# Fresh model from SFT checkpoint\nlean_model = TinyGPT2(VOCAB_SIZE).to(DEVICE)\nlean_model.load_state_dict(sft_model.state_dict())\nlean_ref = copy.deepcopy(lean_model)\nlean_ref.eval()\nfor p in lean_ref.parameters():\n    p.requires_grad = False\n\nlean_optimiser = torch.optim.Adam(lean_model.parameters(), lr=5e-5)\n\nLEAN_STEPS = 40\nLEAN_GROUP_SIZE = 8\nLEAN_MAX_TACTIC_LEN = 12  # tactics are short: \"norm_num\", \"simp\", \"omega\"\n\nlean_reward_history = []\nlean_success_rate_history = []\n\nprint(\"Starting GRPO training with Lean verification rewards...\\n\")\nfor step in range(LEAN_STEPS):\n    step_rewards = []\n    step_verified = 0\n    step_total = 0\n\n    # Cycle through theorems\n    thm_idx = step % len(LEAN_THEOREMS)\n    thm = LEAN_THEOREMS[thm_idx]\n    prompt_ids = lean_prompts[thm_idx]\n\n    # Generate a group of tactic candidates\n    group_data = grpo_generate_group(\n        lean_model, prompt_ids, group_size=LEAN_GROUP_SIZE,\n        max_new_tokens=LEAN_MAX_TACTIC_LEN, temperature=1.0,\n    )\n\n    # Replace heuristic rewards with Lean verification rewards\n    lean_rewards = []\n    for i in range(LEAN_GROUP_SIZE):\n        full_text = tokeniser.decode(group_data[\"sequences\"][i].tolist())\n        # Extract tactic: everything after \" := by \"\n        marker = \" := by \"\n        if marker in full_text:\n            tactic = full_text.split(marker, 1)[1].strip()\n        else:\n            tactic = full_text.strip()\n        reward = compute_lean_reward(thm[\"statement\"], tactic)\n        lean_rewards.append(reward)\n        if reward > 0:\n            step_verified += 1\n        step_total += 1\n\n    rewards_t = torch.tensor(lean_rewards, dtype=torch.float32, device=DEVICE)\n\n    # Recompute group-normalised advantages with Lean rewards\n    mean_r = rewards_t.mean()\n    std_r = rewards_t.std() + 1e-8\n    group_data[\"advantages\"] = (rewards_t - mean_r) / std_r\n    group_data[\"rewards\"] = rewards_t\n\n    # GRPO update\n    loss = grpo_update(\n        lean_model, lean_ref, lean_optimiser, group_data,\n        clip_eps=0.2, beta_kl=0.04, n_epochs=2,\n    )\n\n    avg_reward = mean_r.item()\n    success_rate = step_verified / step_total\n    lean_reward_history.append(avg_reward)\n    lean_success_rate_history.append(success_rate)\n\n    if (step + 1) % 10 == 0:\n        print(\n            f\"Step {step+1:3d}/{LEAN_STEPS}  \"\n            f\"thm={thm['desc'][:20]:20s}  \"\n            f\"reward={avg_reward:.3f}  \"\n            f\"verified={step_verified}/{step_total}  \"\n            f\"loss={loss:.4f}\"\n        )\n\nprint(\"\\nGRPO+Lean training complete.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ---------------------------------------------------------------------------\n# 7e. Visualisation: Reward & Proof Success Rate\n# ---------------------------------------------------------------------------\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# Reward curve\nax1.plot(lean_reward_history, linewidth=1.5, color=\"tab:blue\")\nax1.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\nax1.set_xlabel(\"Step\")\nax1.set_ylabel(\"Mean Reward\")\nax1.set_title(\"GRPO + Lean: Mean Reward over Training\")\nax1.grid(True, alpha=0.3)\n\n# Success rate (smoothed)\nwindow = max(1, len(lean_success_rate_history) // 10)\nif len(lean_success_rate_history) >= window:\n    smoothed = np.convolve(\n        lean_success_rate_history, np.ones(window) / window, mode=\"valid\"\n    )\n    ax2.plot(smoothed, linewidth=1.5, color=\"tab:green\", label=f\"smoothed (w={window})\")\nax2.plot(lean_success_rate_history, alpha=0.3, color=\"tab:green\", label=\"raw\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Proof Success Rate\")\nax2.set_title(\"Fraction of Generated Proofs Verified by Lean\")\nax2.set_ylim(-0.05, 1.05)\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Show some example generations\nprint(\"\\n--- Example generated tactics (last theorem seen) ---\")\nlean_model.eval()\nfor thm in LEAN_THEOREMS[:5]:\n    prompt_text = f\"{thm['statement']} := by \"\n    ids = tokeniser.encode(prompt_text)\n    input_ids = torch.tensor([ids], dtype=torch.long, device=DEVICE)\n    with torch.no_grad():\n        gen = lean_model.generate(input_ids, max_new_tokens=LEAN_MAX_TACTIC_LEN, temperature=0.5)\n    full_text = tokeniser.decode(gen[0].tolist())\n    marker = \" := by \"\n    tactic = full_text.split(marker, 1)[1].strip() if marker in full_text else \"???\"\n    result = verify_lean_code(f\"{thm['statement']} := by {tactic}\")\n    status = \"\" if result[\"verified\"] else \"\"\n    print(f\"  {thm['statement']}  =>  {tactic:15s}  [{status}]  (expected: {thm['tactic']})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Discussion\n",
    "\n",
    "**What we demonstrated:** Even a tiny (~1M parameter) model can learn to associate theorem statements with correct Lean tactics when given binary verification feedback through GRPO. The reward signal is **perfect**  Lean either accepts or rejects a proof, with no ambiguity.\n",
    "\n",
    "**Connection to real systems:**\n",
    "- **DeepSeek-Prover-V2** (2025) uses exactly this pipeline at scale: generate groups of proof candidates, verify with Lean, update with GRPO.\n",
    "- **AlphaProof** (DeepMind, 2024) combines this with Monte Carlo Tree Search (MCTS) over tactic sequences, achieving silver-medal performance at IMO 2024.\n",
    "\n",
    "**Why this matters for finance:** The same principle  *RL with a verifiable reward*  applies whenever we have an automated checker:\n",
    "- **Code generation:** run unit tests as reward\n",
    "- **Quantitative finance:** backtest trading strategies as reward\n",
    "- **Regulatory compliance:** rule-based checkers as reward\n",
    "\n",
    "**Limitations of our toy example:**\n",
    "- The model is far too small to learn generalised proof strategies\n",
    "- We only use single-tactic proofs; real proofs require multi-step reasoning\n",
    "- Training steps are too few for meaningful convergence\n",
    "- The character-level tokeniser is not ideal for Lean syntax\n",
    "- If the public Lean server is unreachable, the training loop falls back to cached results  only the known-correct ground-truth tactics receive reward $+1$, so the model receives $-0.2$ on all generated attempts and the reward curve stays flat. When the server is live, the model can discover correct tactics on its own and the curve rises."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary\n\nThis notebook covered the key post-training methods for language models:\n\n| Method | Requires Reward Model? | Requires Critic? | Key Mechanism |\n|--------|----------------------|------------------|---------------|\n| **SFT** | No | No | Teacher-forced cross-entropy on demonstrations |\n| **REINFORCE** | Yes (or heuristic) | No | $\\nabla J = \\mathbb{E}[\\nabla\\log\\pi \\cdot R]$, high variance |\n| **PPO** | Yes (or heuristic) | Yes (value fn) | Clipped ratio + GAE advantages |\n| **GRPO** | Yes (or heuristic) | **No** | Group-normalised advantages, no critic |\n| **DPO** | **No** | No | Direct optimisation from preferences |\n| **Bradley-Terry RM** | N/A (this *is* the RM) | No | Pairwise preference loss |\n| **GRPO + Lean** | **No** (verifier) | **No** | Formal verification as perfect reward |\n\n**Key takeaways:**\n- SFT provides the foundation; RL methods refine behaviour towards a reward signal.\n- REINFORCE is simple but suffers from high variance; baselines help.\n- PPO stabilises training via clipping and GAE, but requires a critic.\n- GRPO achieves similar results without a critic by using group-relative advantages.\n- DPO bypasses reward modelling entirely, optimising directly from preferences.\n- The Bradley-Terry model provides a principled way to learn rewards from preferences.\n- **Formal verification** (e.g. Lean 4) provides a perfect, non-hackable reward signal for mathematical reasoning  the same GRPO algorithm applies, but with provably correct feedback."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}