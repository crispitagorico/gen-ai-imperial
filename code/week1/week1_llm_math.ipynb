{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: The Mathematics of Large Language Models\n",
    "\n",
    "**MSc Course: Generative Models in Finance** \n",
    "**Imperial College London**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook provides a hands-on introduction to the core building blocks of large language models (LLMs). We will implement each component from scratch, building up from tokenisation to a full decoder-only Transformer.\n",
    "\n",
    "**Contents:**\n",
    "1. Tokenisation (Byte Pair Encoding)\n",
    "2. N-gram Language Models\n",
    "3. Neural Language Models (Feedforward)\n",
    "4. Attention Mechanisms\n",
    "5. Tiny Transformer (Decoder-Only)\n",
    "6. Perplexity Evaluation\n",
    "\n",
    "**Dependencies:** `torch`, `numpy`, `matplotlib`, `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Tokenisation: Byte Pair Encoding (BPE)\n",
    "\n",
    "Tokenisation is the first step in any language model pipeline. It converts raw text into a sequence of discrete tokens (integers) that the model can process.\n",
    "\n",
    "**Byte Pair Encoding (BPE)** is the dominant tokenisation algorithm used in modern LLMs (GPT-2, GPT-3, GPT-4). The algorithm works as follows:\n",
    "\n",
    "1. Start with a vocabulary of individual characters (or bytes).\n",
    "2. Count all adjacent pairs of tokens in the corpus.\n",
    "3. Merge the most frequent pair into a single new token.\n",
    "4. Repeat steps 2-3 for a fixed number of merges.\n",
    "\n",
    "This builds a vocabulary bottom-up, capturing frequent subwords like common suffixes (`-ing`, `-tion`) and frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BPE from scratch ---\n",
    "\n",
    "# A small corpus of financial news sentences\n",
    "corpus = [\n",
    "    \"The Federal Reserve raised interest rates by 25 basis points.\",\n",
    "    \"Stock markets rallied after the interest rate decision was announced.\",\n",
    "    \"The interest rate hike was widely expected by market participants.\",\n",
    "    \"Bond yields fell sharply as investors adjusted their portfolios.\",\n",
    "    \"The central bank signalled further rate increases in the coming months.\",\n",
    "    \"Inflation data released today exceeded market expectations.\",\n",
    "    \"The equity market experienced significant volatility this quarter.\",\n",
    "    \"Treasury yields rose as the market priced in additional rate hikes.\",\n",
    "    \"Investors are closely watching the interest rate trajectory.\",\n",
    "    \"The rate decision impacted both equity and bond markets significantly.\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_pairs(token_list):\n",
    "    \"\"\"Count frequency of all adjacent pairs in a list of token sequences.\"\"\"\n",
    "    pairs = Counter()\n",
    "    for tokens in token_list:\n",
    "        for i in range(len(tokens) - 1):\n",
    "            pairs[(tokens[i], tokens[i + 1])] += 1\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_pair(token_list, pair, new_token):\n",
    "    \"\"\"Merge all occurrences of `pair` into `new_token` in every sequence.\"\"\"\n",
    "    new_token_list = []\n",
    "    for tokens in token_list:\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        new_token_list.append(new_tokens)\n",
    "    return new_token_list\n",
    "\n",
    "\n",
    "def train_bpe(corpus, num_merges=30):\n",
    "    \"\"\"Train BPE on a corpus, returning the merge rules and final vocabulary.\"\"\"\n",
    "    # Initialise: split each word into characters, with a special end-of-word marker\n",
    "    # We treat each sentence as a sequence of characters\n",
    "    token_list = [list(sentence) for sentence in corpus]\n",
    "\n",
    "    merges = []\n",
    "    vocab = set()\n",
    "    for tokens in token_list:\n",
    "        vocab.update(tokens)\n",
    "\n",
    "    print(f\"Initial vocabulary size: {len(vocab)}\")\n",
    "    print(f\"Initial vocabulary: {sorted(vocab)}\\n\")\n",
    "\n",
    "    for step in range(num_merges):\n",
    "        pairs = get_pairs(token_list)\n",
    "        if not pairs:\n",
    "            break\n",
    "\n",
    "        # Find the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        best_count = pairs[best_pair]\n",
    "        new_token = best_pair[0] + best_pair[1]\n",
    "\n",
    "        # Perform the merge\n",
    "        token_list = merge_pair(token_list, best_pair, new_token)\n",
    "        merges.append((best_pair, new_token))\n",
    "        vocab.add(new_token)\n",
    "\n",
    "        if step < 15:  # Print first 15 merges\n",
    "            print(f\"Merge {step + 1:2d}: ('{best_pair[0]}', '{best_pair[1]}') -> '{new_token}'  \"\n",
    "                  f\"(count={best_count})\")\n",
    "\n",
    "    print(f\"\\n... ({num_merges} merges total)\")\n",
    "    print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    return merges, vocab, token_list\n",
    "\n",
    "\n",
    "merges, vocab, tokenised_corpus = train_bpe(corpus, num_merges=30)\n",
    "\n",
    "# Show what the tokenised text looks like\n",
    "print(\"\\n--- Tokenised sentences (first 3) ---\")\n",
    "for i, tokens in enumerate(tokenised_corpus[:3]):\n",
    "    print(f\"Sentence {i + 1}: {tokens}\")\n",
    "    print(f\"  Num tokens: {len(tokens)}  (original chars: {len(corpus[i])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare with tiktoken (cl100k_base, used by GPT-4) ---\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "print(\"Comparison: Our BPE vs tiktoken (cl100k_base)\\n\")\n",
    "print(f\"{'Sentence':<65} {'Our BPE':>10} {'tiktoken':>10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for sentence in corpus[:5]:\n",
    "    our_tokens = tokenised_corpus[corpus.index(sentence)]\n",
    "    tiktoken_ids = enc.encode(sentence)\n",
    "    print(f\"{sentence:<65} {len(our_tokens):>10} {len(tiktoken_ids):>10}\")\n",
    "\n",
    "# Show tiktoken's actual tokens for one sentence\n",
    "example = corpus[0]\n",
    "tiktoken_ids = enc.encode(example)\n",
    "tiktoken_tokens = [enc.decode([tid]) for tid in tiktoken_ids]\n",
    "\n",
    "print(f\"\\n--- tiktoken tokenisation of: '{example}' ---\")\n",
    "print(f\"Token IDs: {tiktoken_ids}\")\n",
    "print(f\"Tokens:    {tiktoken_tokens}\")\n",
    "print(f\"Num tokens: {len(tiktoken_ids)}\")\n",
    "print(f\"Vocabulary size of cl100k_base: {enc.n_vocab:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "- BPE builds a vocabulary bottom-up by iteratively merging the most frequent character pairs.\n",
    "- With only 30 merges on our tiny corpus, BPE already captures common subwords.\n",
    "- Production tokenisers like `cl100k_base` use ~100k merges trained on billions of tokens, producing far more efficient encodings.\n",
    "- Tokenisation determines the fundamental \"atoms\" that a language model sees -- it is a crucial design choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. N-gram Language Models\n",
    "\n",
    "An **n-gram language model** estimates the probability of the next token given the preceding $n-1$ tokens:\n",
    "\n",
    "$$P(w_t \\mid w_{t-n+1}, \\ldots, w_{t-1}) = \\frac{\\text{count}(w_{t-n+1}, \\ldots, w_t)}{\\text{count}(w_{t-n+1}, \\ldots, w_{t-1})}$$\n",
    "\n",
    "We use **Laplace (add-1) smoothing** to handle unseen n-grams:\n",
    "\n",
    "$$P_{\\text{Laplace}}(w_t \\mid w_{t-n+1}, \\ldots, w_{t-1}) = \\frac{\\text{count}(w_{t-n+1}, \\ldots, w_t) + 1}{\\text{count}(w_{t-n+1}, \\ldots, w_{t-1}) + V}$$\n",
    "\n",
    "where $V$ is the vocabulary size.\n",
    "\n",
    "**Perplexity** measures how well the model predicts held-out text:\n",
    "\n",
    "$$\\text{PPL} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i \\mid \\text{context}_i)\\right)$$\n",
    "\n",
    "Lower perplexity means the model assigns higher probability to the observed text (better predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build a character-level n-gram model ---\n",
    "\n",
    "# We use a slightly larger text for more meaningful statistics\n",
    "text_data = \"\"\"The stock market experienced a sharp decline today as investors reacted to\n",
    "rising inflation data. The Federal Reserve is expected to raise interest rates\n",
    "at its next meeting. Bond yields have risen steadily over the past quarter.\n",
    "Market analysts predict continued volatility in the coming weeks. The central\n",
    "bank has signalled its intention to maintain a hawkish stance on monetary policy.\n",
    "Equity markets in Europe and Asia also fell in response to the data. Investors\n",
    "are repositioning their portfolios to hedge against further rate increases.\n",
    "The dollar strengthened against major currencies as rate expectations shifted.\n",
    "Credit spreads widened as risk sentiment deteriorated across asset classes.\n",
    "The yield curve inversion deepened, raising recession concerns among economists.\"\"\"\n",
    "\n",
    "# Preprocessing: character-level\n",
    "chars = sorted(set(text_data))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "V = len(chars)\n",
    "\n",
    "print(f\"Character vocabulary size: {V}\")\n",
    "print(f\"Characters: {chars}\")\n",
    "\n",
    "# Split into train and test\n",
    "split_point = int(0.8 * len(text_data))\n",
    "train_text = text_data[:split_point]\n",
    "test_text = text_data[split_point:]\n",
    "print(f\"\\nTrain length: {len(train_text)} chars\")\n",
    "print(f\"Test length:  {len(test_text)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel:\n",
    "    \"\"\"Character-level n-gram language model with Laplace smoothing.\"\"\"\n",
    "\n",
    "    def __init__(self, n, vocab_size, smoothing=1.0):\n",
    "        self.n = n\n",
    "        self.vocab_size = vocab_size\n",
    "        self.smoothing = smoothing\n",
    "        self.ngram_counts = Counter()    # counts of full n-grams\n",
    "        self.context_counts = Counter()  # counts of (n-1)-gram contexts\n",
    "\n",
    "    def train(self, text):\n",
    "        \"\"\"Count all n-grams and (n-1)-gram contexts in the training text.\"\"\"\n",
    "        for i in range(len(text) - self.n + 1):\n",
    "            ngram = text[i:i + self.n]\n",
    "            context = text[i:i + self.n - 1]\n",
    "            self.ngram_counts[ngram] += 1\n",
    "            self.context_counts[context] += 1\n",
    "\n",
    "        print(f\"{self.n}-gram model trained.\")\n",
    "        print(f\"  Unique {self.n}-grams: {len(self.ngram_counts)}\")\n",
    "        print(f\"  Unique {self.n - 1}-gram contexts: {len(self.context_counts)}\")\n",
    "\n",
    "    def prob(self, char, context):\n",
    "        \"\"\"P(char | context) with Laplace smoothing.\"\"\"\n",
    "        ngram = context + char\n",
    "        count_ngram = self.ngram_counts.get(ngram, 0)\n",
    "        count_context = self.context_counts.get(context, 0)\n",
    "        return (count_ngram + self.smoothing) / (count_context + self.smoothing * self.vocab_size)\n",
    "\n",
    "    def log_prob(self, char, context):\n",
    "        \"\"\"Log probability of char given context.\"\"\"\n",
    "        return math.log(self.prob(char, context))\n",
    "\n",
    "    def perplexity(self, text):\n",
    "        \"\"\"Compute perplexity on a text string.\"\"\"\n",
    "        total_log_prob = 0.0\n",
    "        count = 0\n",
    "        for i in range(self.n - 1, len(text)):\n",
    "            context = text[i - self.n + 1:i]\n",
    "            char = text[i]\n",
    "            total_log_prob += self.log_prob(char, context)\n",
    "            count += 1\n",
    "        avg_log_prob = total_log_prob / count\n",
    "        return math.exp(-avg_log_prob)\n",
    "\n",
    "    def generate(self, seed, length=200):\n",
    "        \"\"\"Generate text by sampling from the model.\"\"\"\n",
    "        assert len(seed) >= self.n - 1, f\"Seed must be at least {self.n - 1} characters.\"\n",
    "        result = list(seed)\n",
    "        for _ in range(length):\n",
    "            context = ''.join(result[-(self.n - 1):])\n",
    "            # Compute probabilities over all characters\n",
    "            probs = []\n",
    "            for ch in chars:\n",
    "                probs.append(self.prob(ch, context))\n",
    "            probs = np.array(probs)\n",
    "            probs /= probs.sum()  # normalise\n",
    "            idx = np.random.choice(len(chars), p=probs)\n",
    "            result.append(chars[idx])\n",
    "        return ''.join(result)\n",
    "\n",
    "\n",
    "# Train bigram (n=2) and trigram (n=3) models\n",
    "bigram_model = NgramModel(n=2, vocab_size=V)\n",
    "bigram_model.train(train_text)\n",
    "\n",
    "print()\n",
    "\n",
    "trigram_model = NgramModel(n=3, vocab_size=V)\n",
    "trigram_model.train(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute perplexity on held-out text ---\n",
    "\n",
    "bigram_ppl = bigram_model.perplexity(test_text)\n",
    "trigram_ppl = trigram_model.perplexity(test_text)\n",
    "\n",
    "print(f\"Bigram  perplexity on test set: {bigram_ppl:.2f}\")\n",
    "print(f\"Trigram perplexity on test set: {trigram_ppl:.2f}\")\n",
    "print(f\"\\n(Lower is better. Random baseline would be {V:.0f}.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate random text from the n-gram models ---\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Bigram model generated text:\")\n",
    "print(\"=\" * 70)\n",
    "print(bigram_model.generate(seed=\"Th\", length=250))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Trigram model generated text:\")\n",
    "print(\"=\" * 70)\n",
    "print(trigram_model.generate(seed=\"The\", length=250))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "- The bigram model achieves lower perplexity than the trigram model on this tiny corpus. Although the trigram uses more context, the data is too sparse for it to benefit: Laplace smoothing spreads probability mass over many unseen trigrams, hurting predictions. With a larger corpus, the trigram would overtake the bigram.\n",
    "- More generally, n-gram models suffer from the **curse of dimensionality**: the number of possible n-grams grows exponentially with $n$, requiring exponentially more data.\n",
    "- Laplace smoothing is a simple way to handle unseen n-grams, but more sophisticated smoothing methods (Kneser-Ney, etc.) are used in practice.\n",
    "- The generated text is locally coherent but globally nonsensical -- the model has no notion of long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Neural Language Model (Feedforward)\n",
    "\n",
    "A **feedforward neural language model** (Bengio et al., 2003) addresses the curse of dimensionality by representing words as dense vectors (embeddings) and using a neural network to predict the next token:\n",
    "\n",
    "1. Map each character in a context window of size $k$ to an embedding vector.\n",
    "2. Concatenate the embeddings.\n",
    "3. Pass through a hidden layer with a nonlinearity (e.g. ReLU).\n",
    "4. Project to vocabulary-size logits and apply softmax.\n",
    "\n",
    "$$P(w_t \\mid w_{t-k}, \\ldots, w_{t-1}) = \\text{softmax}(W_2 \\cdot \\text{ReLU}(W_1 \\cdot [e_{t-k}; \\ldots; e_{t-1}] + b_1) + b_2)$$\n",
    "\n",
    "We will train this on character sequences from our financial text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare character-level dataset ---\n",
    "\n",
    "CONTEXT_SIZE = 8  # Number of preceding characters used as context\n",
    "\n",
    "def make_dataset(text, context_size):\n",
    "    \"\"\"Create (context, target) pairs from text.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(context_size, len(text)):\n",
    "        context = [char_to_idx[ch] for ch in text[i - context_size:i]]\n",
    "        target = char_to_idx[text[i]]\n",
    "        X.append(context)\n",
    "        Y.append(target)\n",
    "    return torch.tensor(X, dtype=torch.long), torch.tensor(Y, dtype=torch.long)\n",
    "\n",
    "X_train, Y_train = make_dataset(train_text, CONTEXT_SIZE)\n",
    "X_test, Y_test = make_dataset(test_text, CONTEXT_SIZE)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples:     {X_test.shape[0]}\")\n",
    "print(f\"Context size:     {CONTEXT_SIZE}\")\n",
    "print(f\"Vocabulary size:  {V}\")\n",
    "print(f\"\\nExample input:  {X_train[0].tolist()} -> {''.join([idx_to_char[i] for i in X_train[0].tolist()])}\")\n",
    "print(f\"Example target: {Y_train[0].item()} -> '{idx_to_char[Y_train[0].item()]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feedforward Neural Language Model ---\n",
    "\n",
    "class FeedforwardLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, context_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim * context_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, context_size)\n",
    "        emb = self.embedding(x)           # (batch, context_size, embed_dim)\n",
    "        emb = emb.view(emb.size(0), -1)   # (batch, context_size * embed_dim)\n",
    "        h = F.relu(self.fc1(emb))          # (batch, hidden_dim)\n",
    "        h = F.relu(self.fc2(h))            # (batch, hidden_dim)\n",
    "        logits = self.fc3(h)               # (batch, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "EMBED_DIM = 32\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "ffn_model = FeedforwardLM(V, EMBED_DIM, CONTEXT_SIZE, HIDDEN_DIM).to(device)\n",
    "num_params = sum(p.numel() for p in ffn_model.parameters())\n",
    "print(f\"Feedforward LM parameters: {num_params:,}\")\n",
    "print(ffn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the feedforward LM ---\n",
    "\n",
    "optimizer = torch.optim.Adam(ffn_model.parameters(), lr=1e-3)\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "losses = []\n",
    "X_tr = X_train.to(device)\n",
    "Y_tr = Y_train.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(X_tr.size(0))\n",
    "    X_shuffled = X_tr[perm]\n",
    "    Y_shuffled = Y_tr[perm]\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i in range(0, X_tr.size(0), BATCH_SIZE):\n",
    "        xb = X_shuffled[i:i + BATCH_SIZE]\n",
    "        yb = Y_shuffled[i:i + BATCH_SIZE]\n",
    "\n",
    "        logits = ffn_model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    if (epoch + 1) % 40 == 0:\n",
    "        print(f\"Epoch {epoch + 1:3d}/{EPOCHS}  Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal training loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot the training loss curve ---\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(losses, linewidth=1.5, color='#2c7bb6')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Feedforward Neural LM: Training Loss', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate text from the feedforward LM ---\n",
    "\n",
    "def generate_ffn(model, seed, length=200, temperature=0.8):\n",
    "    \"\"\"Generate text from the feedforward LM by sampling.\"\"\"\n",
    "    model.eval()\n",
    "    assert len(seed) >= CONTEXT_SIZE\n",
    "    result = list(seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            context = [char_to_idx[ch] for ch in result[-CONTEXT_SIZE:]]\n",
    "            x = torch.tensor([context], dtype=torch.long).to(device)\n",
    "            logits = model(x)\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            idx = torch.multinomial(probs, 1).item()\n",
    "            result.append(idx_to_char[idx])\n",
    "\n",
    "    return ''.join(result)\n",
    "\n",
    "seed_text = train_text[:CONTEXT_SIZE]\n",
    "print(f\"Seed: '{seed_text}'\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Feedforward LM generated text (temperature=0.8):\")\n",
    "print(\"=\" * 70)\n",
    "print(generate_ffn(ffn_model, seed_text, length=300, temperature=0.8))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Feedforward LM generated text (temperature=0.5):\")\n",
    "print(\"=\" * 70)\n",
    "print(generate_ffn(ffn_model, seed_text, length=300, temperature=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "- The neural LM learns dense representations of characters and can generalise to unseen contexts.\n",
    "- Lower temperature produces more deterministic (repetitive) text; higher temperature produces more diverse (but noisier) text.\n",
    "- The fixed context window is a fundamental limitation: the model cannot attend to information beyond $k$ characters. This motivates the attention mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Attention Mechanism\n",
    "\n",
    "The **scaled dot-product attention** (Vaswani et al., 2017) computes a weighted sum of value vectors, where the weights come from the compatibility (dot product) between query and key vectors:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "where:\n",
    "- $Q \\in \\mathbb{R}^{n \\times d_k}$ are query vectors,\n",
    "- $K \\in \\mathbb{R}^{n \\times d_k}$ are key vectors,\n",
    "- $V \\in \\mathbb{R}^{n \\times d_v}$ are value vectors,\n",
    "- $d_k$ is the dimension of keys (used for scaling).\n",
    "\n",
    "**Causal masking** ensures that position $i$ can only attend to positions $j \\leq i$ (auto-regressive property). This is achieved by setting the upper-triangular entries of the attention score matrix to $-\\infty$ before softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scaled dot-product attention from scratch ---\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: (batch, seq_len, d_k) query vectors\n",
    "        K: (batch, seq_len, d_k) key vectors\n",
    "        V: (batch, seq_len, d_v) value vectors\n",
    "        mask: optional (seq_len, seq_len) boolean mask, True = masked (ignored)\n",
    "\n",
    "    Returns:\n",
    "        output: (batch, seq_len, d_v) weighted sum of values\n",
    "        attn_weights: (batch, seq_len, seq_len) attention weights\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "\n",
    "    # Step 1: Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)  # (batch, seq, seq)\n",
    "\n",
    "    # Step 2: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "\n",
    "    # Step 3: Softmax to get attention weights\n",
    "    attn_weights = F.softmax(scores, dim=-1)  # (batch, seq, seq)\n",
    "\n",
    "    # Step 4: Weighted sum of values\n",
    "    output = torch.matmul(attn_weights, V)  # (batch, seq, d_v)\n",
    "\n",
    "    return output, attn_weights\n",
    "\n",
    "\n",
    "# --- Demonstration with a small example ---\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 1\n",
    "seq_len = 6\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q_demo = torch.randn(batch_size, seq_len, d_k)\n",
    "K_demo = torch.randn(batch_size, seq_len, d_k)\n",
    "V_demo = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "# --- Without masking ---\n",
    "output, attn_weights = scaled_dot_product_attention(Q_demo, K_demo, V_demo)\n",
    "print(\"Attention output shape:\", output.shape)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)\n",
    "print(\"\\nAttention weights (no mask):\")\n",
    "print(attn_weights[0].detach().numpy().round(3))\n",
    "print(\"\\nRow sums (should be 1.0):\", attn_weights[0].sum(dim=-1).detach().numpy().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Causal masking ---\n",
    "\n",
    "# Create a causal mask: True where we want to mask (upper triangle)\n",
    "causal_mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
    "print(\"Causal mask (True = masked):\")\n",
    "print(causal_mask.int().numpy())\n",
    "\n",
    "output_causal, attn_weights_causal = scaled_dot_product_attention(Q_demo, K_demo, V_demo, mask=causal_mask)\n",
    "\n",
    "print(\"\\nAttention weights (with causal mask):\")\n",
    "print(attn_weights_causal[0].detach().numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualise attention weights as heatmaps ---\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "labels = [f\"pos {i}\" for i in range(seq_len)]\n",
    "\n",
    "# Plot 1: No mask\n",
    "im1 = axes[0].imshow(attn_weights[0].detach().numpy(), cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "axes[0].set_title('Attention Weights (No Mask)', fontsize=13)\n",
    "axes[0].set_xlabel('Key position', fontsize=11)\n",
    "axes[0].set_ylabel('Query position', fontsize=11)\n",
    "axes[0].set_xticks(range(seq_len))\n",
    "axes[0].set_yticks(range(seq_len))\n",
    "axes[0].set_xticklabels(labels, fontsize=9)\n",
    "axes[0].set_yticklabels(labels, fontsize=9)\n",
    "plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = attn_weights[0][i, j].item()\n",
    "        axes[0].text(j, i, f\"{val:.2f}\", ha='center', va='center', fontsize=8,\n",
    "                     color='white' if val > 0.5 else 'black')\n",
    "\n",
    "# Plot 2: Causal mask\n",
    "im2 = axes[1].imshow(attn_weights_causal[0].detach().numpy(), cmap='Reds', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_title('Attention Weights (Causal Mask)', fontsize=13)\n",
    "axes[1].set_xlabel('Key position', fontsize=11)\n",
    "axes[1].set_ylabel('Query position', fontsize=11)\n",
    "axes[1].set_xticks(range(seq_len))\n",
    "axes[1].set_yticks(range(seq_len))\n",
    "axes[1].set_xticklabels(labels, fontsize=9)\n",
    "axes[1].set_yticklabels(labels, fontsize=9)\n",
    "plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = attn_weights_causal[0][i, j].item()\n",
    "        axes[1].text(j, i, f\"{val:.2f}\", ha='center', va='center', fontsize=8,\n",
    "                     color='white' if val > 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verify causal property ---\n",
    "\n",
    "print(\"Verifying causal attention property:\")\n",
    "print(\"Each query position should only attend to current and previous positions.\\n\")\n",
    "\n",
    "for i in range(seq_len):\n",
    "    weights = attn_weights_causal[0, i, :].detach().numpy()\n",
    "    future_weight = weights[i + 1:].sum()\n",
    "    past_weight = weights[:i + 1].sum()\n",
    "    print(f\"  Query pos {i}: attends to pos 0..{i}  \"\n",
    "          f\"(weight on past+current: {past_weight:.4f}, weight on future: {future_weight:.4f})\")\n",
    "\n",
    "print(\"\\nAll future weights are 0.0 -- causal masking is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "- Scaled dot-product attention computes a soft lookup: each query \"searches\" over keys and retrieves a weighted combination of values.\n",
    "- The $1/\\sqrt{d_k}$ scaling prevents dot products from growing too large, which would push softmax into saturated regions.\n",
    "- Causal masking enforces the autoregressive property: each position can only \"see\" itself and earlier positions.\n",
    "- This is the fundamental building block of Transformer decoders (GPT-style models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Tiny Transformer (Decoder-Only)\n",
    "\n",
    "We now build a minimal **decoder-only Transformer** from scratch. This is the same architecture used by GPT-2, GPT-3, GPT-4, and most modern LLMs.\n",
    "\n",
    "**Architecture components:**\n",
    "1. **Token embeddings**: Map each token to a dense vector.\n",
    "2. **Sinusoidal positional encodings**: Inject position information.\n",
    "3. **Multi-head causal self-attention**: Attend to previous positions in parallel across multiple heads.\n",
    "4. **Position-wise feedforward network**: Two linear layers with ReLU activation.\n",
    "5. **Layer normalisation**: Stabilise training (we use pre-norm, as in GPT-2).\n",
    "6. **Residual connections**: Enable gradient flow through deep networks.\n",
    "\n",
    "**Model configuration (~400k parameters):**\n",
    "- `d_model = 128` (embedding dimension)\n",
    "- `n_heads = 4` (attention heads)\n",
    "- `n_layers = 2` (Transformer blocks)\n",
    "- `d_ff = 512` (feedforward hidden dimension)\n",
    "- `max_seq_len = 128` (maximum sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sinusoidal Positional Encoding ---\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding as in 'Attention is All You Need'.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "print(\"SinusoidalPositionalEncoding defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Multi-Head Causal Self-Attention ---\n",
    "\n",
    "class MultiHeadCausalSelfAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Register causal mask as buffer\n",
    "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len, dtype=torch.bool), diagonal=1)\n",
    "        self.register_buffer('causal_mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # batch, seq_len, d_model\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (B, T, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Reshape for multi-head: (B, n_heads, T, d_k)\n",
    "        Q = Q.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(B, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)  # (B, n_heads, T, T)\n",
    "\n",
    "        # Apply causal mask\n",
    "        scores = scores.masked_fill(self.causal_mask[:T, :T].unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (B, n_heads, T, T)\n",
    "\n",
    "        # Weighted sum\n",
    "        out = torch.matmul(attn_weights, V)  # (B, n_heads, T, d_k)\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)  # (B, T, d_model)\n",
    "        out = self.W_o(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "print(\"MultiHeadCausalSelfAttention defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transformer Block (Pre-Norm) ---\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single Transformer decoder block with pre-norm architecture.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadCausalSelfAttention(d_model, n_heads, max_seq_len)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm residual connections\n",
    "        x = x + self.attn(self.ln1(x))   # residual + attention\n",
    "        x = x + self.ffn(self.ln2(x))    # residual + feedforward\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Full Decoder-Only Transformer ---\n",
    "\n",
    "class TinyTransformer(nn.Module):\n",
    "    \"\"\"A minimal decoder-only Transformer language model.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Token embedding + positional encoding\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = SinusoidalPositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, max_seq_len)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm and output projection\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Weight tying: share weights between token embedding and output projection\n",
    "        self.head.weight = self.token_emb.weight\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialise weights following GPT-2 conventions.\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: (batch, seq_len) token indices\n",
    "        Returns:\n",
    "            logits: (batch, seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.max_seq_len, f\"Sequence length {T} exceeds max {self.max_seq_len}\"\n",
    "\n",
    "        # Token embeddings scaled by sqrt(d_model)\n",
    "        x = self.token_emb(idx) * math.sqrt(self.d_model)  # (B, T, d_model)\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = self.pos_enc(x)  # (B, T, d_model)\n",
    "\n",
    "        # Pass through Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final layer norm and project to vocabulary\n",
    "        x = self.ln_f(x)          # (B, T, d_model)\n",
    "        logits = self.head(x)     # (B, T, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"TinyTransformer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate the model ---\n",
    "\n",
    "# Model hyperparameters (~400k params)\n",
    "D_MODEL = 128\n",
    "N_HEADS = 4\n",
    "D_FF = 512\n",
    "N_LAYERS = 2\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "transformer = TinyTransformer(\n",
    "    vocab_size=V,\n",
    "    d_model=D_MODEL,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    n_layers=N_LAYERS,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in transformer.parameters())\n",
    "# Note: weight tying means the embedding and head share parameters.\n",
    "# Count unique parameters:\n",
    "unique_params = sum(p.numel() for p in transformer.parameters()) - transformer.token_emb.weight.numel()\n",
    "# The token_emb weights are counted in head.weight too, so total unique = total - one copy\n",
    "# Actually, since they share the same tensor, sum() counts it only once. Let's verify:\n",
    "param_ids = set()\n",
    "unique_count = 0\n",
    "for p in transformer.parameters():\n",
    "    if id(p) not in param_ids:\n",
    "        param_ids.add(id(p))\n",
    "        unique_count += p.numel()\n",
    "\n",
    "print(f\"Unique parameters: {unique_count:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(transformer)\n",
    "\n",
    "# Verify with a dummy forward pass\n",
    "dummy_input = torch.randint(0, V, (2, 16)).to(device)\n",
    "dummy_output = transformer(dummy_input)\n",
    "print(f\"\\nDummy forward pass: input {dummy_input.shape} -> output {dummy_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare training data for the Transformer ---\n",
    "\n",
    "# We use a slightly larger dataset by repeating and augmenting our financial text\n",
    "# Split: 80% train, 20% test\n",
    "split = int(0.8 * len(text_data))\n",
    "text_train = text_data[:split] * 5 # Repeat to give the model enough data\n",
    "text_test = text_data[split:]\n",
    "\n",
    "# Encode the full text as a sequence of character indices\n",
    "train_data = torch.tensor([char_to_idx[ch] for ch in text_train], dtype=torch.long)\n",
    "test_data = torch.tensor([char_to_idx[ch] for ch in text_test], dtype=torch.long)\n",
    "\n",
    "#print(f\"Total encoded length: {len(encoded)}\")\n",
    "print(f\"Train length: {len(train_data)}\")\n",
    "print(f\"Test length:  {len(test_data)}\")\n",
    "\n",
    "\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    \"\"\"Sample a random batch of (input, target) sequences.\"\"\"\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Test the batch function\n",
    "xb, yb = get_batch(train_data, batch_size=4, block_size=32)\n",
    "print(f\"\\nBatch shapes: x={xb.shape}, y={yb.shape}\")\n",
    "print(f\"Example x[0]: {''.join([idx_to_char[i.item()] for i in xb[0]])}\")\n",
    "print(f\"Example y[0]: {''.join([idx_to_char[i.item()] for i in yb[0]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   900 | Train loss: 0.1643 | Val loss: 0.1576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1200 | Train loss: 0.1089 | Val loss: 0.1099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1500 | Train loss: 0.0883 | Val loss: 0.0911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  1800 | Train loss: 0.0825 | Val loss: 0.0800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2100 | Train loss: 0.0755 | Val loss: 0.0767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2400 | Train loss: 0.0759 | Val loss: 0.0744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2700 | Train loss: 0.0736 | Val loss: 0.0725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  2999 | Train loss: 0.0721 | Val loss: 0.0715\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Train the Transformer ---\n",
    "\n",
    "BLOCK_SIZE = 64       # Context window for the Transformer\n",
    "BATCH_SIZE = 32\n",
    "LR = 3e-4\n",
    "NUM_STEPS = 3000\n",
    "EVAL_INTERVAL = 300\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=LR)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "steps_list = []\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data, num_batches=20):\n",
    "    \"\"\"Estimate average loss over several random batches.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for _ in range(num_batches):\n",
    "        xb, yb = get_batch(data, BATCH_SIZE, BLOCK_SIZE)\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits.view(-1, V), yb.view(-1))\n",
    "        total_loss += loss.item()\n",
    "    model.train()\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "transformer.train()\n",
    "print(f\"Training Transformer for {NUM_STEPS} steps...\\n\")\n",
    "\n",
    "for step in range(NUM_STEPS):\n",
    "    xb, yb = get_batch(train_data, BATCH_SIZE, BLOCK_SIZE)\n",
    "\n",
    "    logits = transformer(xb)  # (B, T, V)\n",
    "    loss = F.cross_entropy(logits.view(-1, V), yb.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Gradient clipping for stable training\n",
    "    torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % EVAL_INTERVAL == 0 or step == NUM_STEPS - 1:\n",
    "        train_loss = estimate_loss(transformer, train_data)\n",
    "        val_loss = estimate_loss(transformer, test_data)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        steps_list.append(step)\n",
    "        print(f\"Step {step:5d} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot training and validation loss ---\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(steps_list, train_losses, 'o-', label='Train loss', color='#2c7bb6', linewidth=1.5)\n",
    "ax.plot(steps_list, val_losses, 's-', label='Val loss', color='#d7191c', linewidth=1.5)\n",
    "ax.set_xlabel('Training step', fontsize=12)\n",
    "ax.set_ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "ax.set_title('Tiny Transformer: Training and Validation Loss', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate samples from the trained Transformer ---\n",
    "\n",
    "def generate_transformer(model, seed_text, max_new_tokens=300, temperature=0.8):\n",
    "    \"\"\"Generate text from the Transformer by autoregressive sampling.\"\"\"\n",
    "    model.eval()\n",
    "    idx = torch.tensor([[char_to_idx[ch] for ch in seed_text]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop to last BLOCK_SIZE tokens if needed\n",
    "            idx_cond = idx[:, -BLOCK_SIZE:] if idx.size(1) > BLOCK_SIZE else idx\n",
    "\n",
    "            logits = model(idx_cond)          # (1, T, V)\n",
    "            logits = logits[:, -1, :]          # (1, V) -- logits for last position\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, 1)  # (1, 1)\n",
    "            idx = torch.cat([idx, next_idx], dim=1)  # (1, T+1)\n",
    "\n",
    "    tokens = idx[0].tolist()\n",
    "    return ''.join([idx_to_char[t] for t in tokens])\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Transformer generated text (temperature=0.8):\")\n",
    "print(\"=\" * 70)\n",
    "print(generate_transformer(transformer, \"The market \", max_new_tokens=300, temperature=0.8))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Transformer generated text (temperature=0.5):\")\n",
    "print(\"=\" * 70)\n",
    "print(generate_transformer(transformer, \"The market \", max_new_tokens=300, temperature=0.5))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Transformer generated text (temperature=1.0):\")\n",
    "print(\"=\" * 70)\n",
    "print(generate_transformer(transformer, \"Interest rate\", max_new_tokens=300, temperature=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualise the sinusoidal positional encoding matrix ---\n",
    "\n",
    "pe_module = SinusoidalPositionalEncoding(D_MODEL, max_len=MAX_SEQ_LEN)\n",
    "pe_matrix = pe_module.pe[0].numpy()  # (max_len, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Full positional encoding matrix\n",
    "im = axes[0].imshow(pe_matrix, cmap='RdBu', aspect='auto', interpolation='nearest')\n",
    "axes[0].set_xlabel('Embedding dimension', fontsize=11)\n",
    "axes[0].set_ylabel('Position', fontsize=11)\n",
    "axes[0].set_title('Sinusoidal Positional Encoding Matrix', fontsize=13)\n",
    "plt.colorbar(im, ax=axes[0], fraction=0.046)\n",
    "\n",
    "# Plot individual sinusoidal curves for selected dimensions\n",
    "positions = np.arange(MAX_SEQ_LEN)\n",
    "dims_to_plot = [0, 1, 4, 5, 16, 17, 32, 33]\n",
    "for d in dims_to_plot:\n",
    "    label = f\"dim {d} ({'sin' if d % 2 == 0 else 'cos'})\"\n",
    "    axes[1].plot(positions, pe_matrix[:, d], label=label, alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel('Position', fontsize=11)\n",
    "axes[1].set_ylabel('Encoding value', fontsize=11)\n",
    "axes[1].set_title('Positional Encoding: Selected Dimensions', fontsize=13)\n",
    "axes[1].legend(fontsize=8, loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show that positional encodings have a nice distance structure\n",
    "print(\"\\nCosine similarity between positional encodings:\")\n",
    "print(\"(Nearby positions should be more similar)\\n\")\n",
    "\n",
    "pe_tensor = torch.tensor(pe_matrix[:20])  # First 20 positions\n",
    "pe_normed = pe_tensor / pe_tensor.norm(dim=-1, keepdim=True)\n",
    "cos_sim = torch.mm(pe_normed, pe_normed.T)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5.5))\n",
    "im = ax.imshow(cos_sim.numpy(), cmap='viridis', aspect='auto')\n",
    "ax.set_xlabel('Position', fontsize=11)\n",
    "ax.set_ylabel('Position', fontsize=11)\n",
    "ax.set_title('Cosine Similarity of Positional Encodings (pos 0-19)', fontsize=13)\n",
    "plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways:**\n",
    "- The Transformer combines embeddings, positional encodings, multi-head attention, feedforward layers, residual connections, and layer normalisation.\n",
    "- Even with ~400k parameters and a tiny training set, the Transformer learns to produce coherent financial text.\n",
    "- Sinusoidal positional encodings have a natural distance structure: nearby positions have more similar encodings.\n",
    "- The sinusoidal waves at different frequencies allow the model to capture both fine-grained and coarse positional information.\n",
    "- Weight tying (sharing embedding and output projection weights) reduces parameters and can improve generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Perplexity Evaluation\n",
    "\n",
    "We now compute perplexity for both the Transformer and the n-gram models on the same held-out text, enabling a direct comparison.\n",
    "\n",
    "Recall that perplexity is:\n",
    "\n",
    "$$\\text{PPL} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i \\mid \\text{context}_i)\\right)$$\n",
    "\n",
    "where $N$ is the number of predicted tokens. Lower perplexity indicates a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute Transformer perplexity on held-out text ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def transformer_perplexity(model, data, block_size):\n",
    "    \"\"\"Compute perplexity of the Transformer on encoded data.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Process in non-overlapping chunks\n",
    "    for i in range(0, len(data) - block_size - 1, block_size):\n",
    "        x = data[i:i + block_size].unsqueeze(0).to(device)\n",
    "        y = data[i + 1:i + block_size + 1].unsqueeze(0).to(device)\n",
    "\n",
    "        logits = model(x)  # (1, block_size, V)\n",
    "        loss = F.cross_entropy(logits.view(-1, V), y.view(-1), reduction='sum')\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_tokens += y.numel()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity, avg_loss\n",
    "\n",
    "\n",
    "# Compute perplexities\n",
    "trans_ppl, trans_loss = transformer_perplexity(transformer, test_data, BLOCK_SIZE)\n",
    "\n",
    "# Re-compute n-gram perplexities on the same test text\n",
    "# (We need to use the original test_text string, not the repeated one)\n",
    "# The test_data for the Transformer covers the last 20% of the repeated text.\n",
    "# For a fair comparison, let's compute on the shared test_text.\n",
    "bigram_ppl_eval = bigram_model.perplexity(test_text)\n",
    "trigram_ppl_eval = trigram_model.perplexity(test_text)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERPLEXITY COMPARISON ON HELD-OUT TEXT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<30} {'Perplexity':>15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Random baseline':<30} {V:>15.2f}\")\n",
    "print(f\"{'Bigram (n=2, Laplace)':<30} {bigram_ppl_eval:>15.2f}\")\n",
    "print(f\"{'Trigram (n=3, Laplace)':<30} {trigram_ppl_eval:>15.2f}\")\n",
    "print(f\"{'Tiny Transformer (2-layer)':<30} {trans_ppl:>15.2f}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nThe Transformer achieves the lowest perplexity because it can\")\n",
    "print(f\"attend to a much longer context ({BLOCK_SIZE} characters) and learn\")\n",
    "print(f\"non-linear patterns in the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualise the perplexity comparison ---\n",
    "\n",
    "models = ['Random\\nbaseline', 'Bigram\\n(n=2)', 'Trigram\\n(n=3)', 'Transformer\\n(2-layer)']\n",
    "perplexities = [V, bigram_ppl_eval, trigram_ppl_eval, trans_ppl]\n",
    "colors = ['#999999', '#fdae61', '#f46d43', '#2c7bb6']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart (linear scale)\n",
    "bars = axes[0].bar(models, perplexities, color=colors, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_ylabel('Perplexity', fontsize=12)\n",
    "axes[0].set_title('Model Perplexity Comparison (Linear Scale)', fontsize=13)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, perplexities):\n",
    "    axes[0].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,\n",
    "                 f\"{val:.1f}\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Bar chart (log scale)\n",
    "bars = axes[1].bar(models, perplexities, color=colors, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_ylabel('Perplexity (log scale)', fontsize=12)\n",
    "axes[1].set_title('Model Perplexity Comparison (Log Scale)', fontsize=13)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars, perplexities):\n",
    "    axes[1].text(bar.get_x() + bar.get_width() / 2, bar.get_height() * 1.1,\n",
    "                 f\"{val:.1f}\", ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(f\"  - The Transformer achieves {V / trans_ppl:.1f}x lower perplexity than random guessing.\")\n",
    "print(f\"  - The Transformer achieves {bigram_ppl_eval / trans_ppl:.1f}x lower perplexity than the bigram model.\")\n",
    "print(f\"  - Even the trigram model, which only uses 2 characters of context, does much\")\n",
    "print(f\"    better than random, showing that character sequences are highly predictable.\")\n",
    "print(f\"  - The Transformer's ability to attend to {BLOCK_SIZE} characters of context gives\")\n",
    "print(f\"    it a significant advantage over fixed-window n-gram models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this notebook, we have built up the core components of a large language model from scratch:\n",
    "\n",
    "| Component | Key Idea |\n",
    "|-----------|----------|\n",
    "| **BPE Tokenisation** | Bottom-up vocabulary construction by iteratively merging frequent pairs |\n",
    "| **N-gram Models** | Count-based probability estimation with smoothing; limited context window |\n",
    "| **Neural LM (Feedforward)** | Dense embeddings + neural network overcome the curse of dimensionality |\n",
    "| **Attention** | Soft lookup mechanism; scaled dot-product; causal masking for autoregression |\n",
    "| **Transformer** | Combines attention, FFN, residual connections, layer norm into a powerful architecture |\n",
    "| **Perplexity** | Standard evaluation metric; measures how well the model predicts held-out text |\n",
    "\n",
    "**Next week:** We will explore how to scale these ideas to larger models and study fine-tuning, RLHF, and the training pipeline of modern LLMs in the context of financial applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
